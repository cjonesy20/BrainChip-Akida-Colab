{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!wget https://doc.brainchipinc.com/_downloads/0792bc3dc7b01941f86b4f993c20ab5f/requirements.txt\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# Upgrading to Akida 2.0\n",
        "\n",
        "This tutorial targets Akida 1.0 users that are looking for advice on how to migrate their Akida 1.0\n",
        "model towards Akida 2.0. It also lists the major differences in model architecture compatibilities\n",
        "between 1.0 and 2.0.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Workflow differences\n",
        "\n",
        ".. figure:: ../../img/1.0vs2.0_flow.png\n",
        "   :target: ../../_images/1.0vs2.0_flow.png\n",
        "   :alt: 1.0 vs. 2.0 flow\n",
        "   :scale: 25 %\n",
        "   :align: center\n",
        "\n",
        "   Akida 1.0 and 2.0 workflows\n",
        "\n",
        "As shown in the figure above, the main difference between 1.0 and 2.0 workflows is the\n",
        "quantization step that was based on CNN2SNN and that is now based on QuantizeML.\n",
        "\n",
        "Providing your model architecture is 2.0 compatible ([next section](plot_1_upgrading_to_2.0.html#models-architecture-differences)_ lists differences), upgrading to\n",
        "2.0 is limited to moving from a [cnn2snn.quantize](../../api_reference/cnn2snn_apis.html#cnn2snn.quantize)_ call to a [quantizeml.models.quantize](../../api_reference/quantizeml_apis.html#quantizeml.models.quantize)_ call. The code snippets\n",
        "below show the two different calls.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "\n",
        "# Build a simple model that is cross-compatible\n",
        "input = keras.layers.Input((32, 32, 3))\n",
        "x = keras.layers.Conv2D(kernel_size=3, filters=32, strides=2, padding='same')(input)\n",
        "x = keras.layers.BatchNormalization()(x)\n",
        "x = keras.layers.ReLU(max_value=6.0)(x)\n",
        "x = keras.layers.Flatten()(x)\n",
        "x = keras.layers.Dense(units=10)(x)\n",
        "\n",
        "model = keras.Model(input, x)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import cnn2snn\n",
        "\n",
        "# Akida 1.0 flow\n",
        "quantized_model_1_0 = cnn2snn.quantize(model, input_weight_quantization=8, weight_quantization=4,\n",
        "                                       activ_quantization=4)\n",
        "akida_model_1_0 = cnn2snn.convert(quantized_model_1_0)\n",
        "akida_model_1_0.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import quantizeml\n",
        "\n",
        "# Akida 2.0 flow\n",
        "qparams = quantizeml.layers.QuantizationParams(input_weight_bits=8, weight_bits=4,\n",
        "                                               activation_bits=4)\n",
        "quantized_model_2_0 = quantizeml.models.quantize(model, qparams=qparams)\n",
        "akida_model_2_0 = cnn2snn.convert(quantized_model_2_0)\n",
        "akida_model_2_0.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-info\"><h4>Note</h4><p>Here we use 8/4/4 quantization to match the CNN2SNN version above, but most users will\n",
        "          typically use the default 8-bit quantization that comes with QuantizeML.</p></div>\n",
        "\n",
        "QuantizeML quantization API is close to the legacy CNN2SNN quantization API and further details on\n",
        "how to use it are given in the [global workflow tutorial](../general/plot_0_global_workflow.html)_ and the [advanced QuantizeML tutorial](plot_0_advanced_quantizeml.html)_.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Models architecture differences\n",
        "\n",
        "### 2.1. Separable convolution\n",
        "\n",
        "In Akida 1.0, a Keras [SeparableConv2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/SeparableConv2D)_ used to be\n",
        "quantized as a [QuantizedSeparableConv2D](../../api_reference/cnn2snn_apis.html#cnn2snn.QuantizedSeparableConv2D)_ and converted to an\n",
        "Akida [SeparableConvolutional](../../api_reference/akida_apis.html#akida.SeparableConvolutional)_ layer. These 3 layers each\n",
        "perform a \"fused\" operation where the depthwise and pointwise operations are grouped together in a\n",
        "single layer.\n",
        "\n",
        "In Akida 2.0, the fused separable layer support has been dropped in favor of a more commonly used\n",
        "unfused operation where the depthwise and the pointwise operations are computed in independent\n",
        "layers. The akida_models package offers a [separable_conv_block](../../api_reference/akida_models_apis.html#akida_models.layer_blocks.separable_conv_block)_\n",
        "with a ``fused=False`` parameter that will create the [DepthwiseConv2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/DepthwiseConv2D)_ and the pointwise\n",
        "[Conv2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D)_ layers under the\n",
        "hood. This block will then be quantized towards a [QuantizedDepthwiseConv2D](../../api_reference/quantizeml_apis.html#quantizeml.layers.QuantizedDepthwiseConv2D)_ and a\n",
        "pointwise [QuantizedConv2D](../../api_reference/quantizeml_apis.html#quantizeml.layers.QuantizedConv2D)_ before\n",
        "conversion into [DepthwiseConv2D](../../api_reference/akida_apis.html#akida.DepthwiseConv2D)_\n",
        "and pointwise [Conv2D](../../api_reference/akida_apis.html#akida.Conv2D)_ respectively.\n",
        "\n",
        "Note that while the resulting model topography is slightly different, the fused and unfused\n",
        "mathematical operations are strictly equivalent.\n",
        "\n",
        "In order to ease 1.0 to 2.0 migration of existing models, akida_models offers an\n",
        "[unfuse_sepconv2d](../../api_reference/akida_models_apis.html#akida_models.unfuse_sepconv2d)_\n",
        "API that takes a model with fused layers and transforms it into an unfused equivalent version. For\n",
        "convenience, an ``unfuse`` CLI action is also provided.\n",
        "\n",
        "```bash\n",
        "akida_models unfuse -m model.h5\n",
        "```\n",
        "### 2.2. Global average pooling operation\n",
        "\n",
        "The supported position of the [GlobalAveragePooling2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/GlobalAveragePooling2D)_ operation\n",
        "has changed in Akida 2.0 as it now must come after the ReLU activation (when there is one). In\n",
        "other words, in Akida 1.0 the layers were organized as follows:\n",
        "\n",
        "* ... > Neural layer > GlobalAveragePooling > (BatchNormalization) > ReLU >  Neural layer > ...\n",
        "\n",
        "In Akida 2.0 the supported sequence of layer is:\n",
        "\n",
        "* ... > Neural layer > (BatchNormalization) > (ReLU) > GlobalAveragePooling >  Neural layer > ...\n",
        "\n",
        "This can also be configured using the ``post_relu_gap`` parameter of akida_models [layer_blocks](../../api_reference/akida_models_apis.html#layer-blocks)_.\n",
        "\n",
        "To migrate an existing model from 1.0 to 2.0, it is possible to load 1.0 weights into a 2.0\n",
        "oriented architecture using [Keras save and load APIs](https://www.tensorflow.org/tutorials/keras/save_and_load)_ because the global average pooling\n",
        "position does not have an effect on model weights. However, the sequences between 1.0 and 2.0 are\n",
        "not mathematically equivalent so it might be required to tune or even retrain the model.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Using ``AkidaVersion``\n",
        "\n",
        "It is still possible to build, quantize and convert models towards a 1.0 target using the\n",
        "[AkidaVersion API](../../api_reference/cnn2snn_apis.html#akida-version)_.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Reusing the previously defined 2.0 model but converting to a 1.0 target this time\n",
        "with cnn2snn.set_akida_version(cnn2snn.AkidaVersion.v1):\n",
        "    akida_model = cnn2snn.convert(quantized_model_2_0)\n",
        "akida_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "One will notice the different Akida layers types as detailed in [Akida user guide](../../user_guide/akida.html#akida-layers)_.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

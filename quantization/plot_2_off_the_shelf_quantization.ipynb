{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!wget https://doc.brainchipinc.com/_downloads/0792bc3dc7b01941f86b4f993c20ab5f/requirements.txt\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# Off-the-shelf models quantization\n",
        "\n",
        ".. Warning::\n",
        "   QuantizeML ONNX quantization is an **evolving feature**. Some models may not be compatible.\n",
        "\n",
        "| The [Global Akida workflow](../general/plot_0_global_workflow.html)_ and the\n",
        "  [PyTorch to Akida workflow](../general/plot_8_global_pytorch_workflow.html)_ guides\n",
        "  describe all the steps required to create, train, quantize and convert a model for Akida,\n",
        "  respectively using TensorFlow/Keras and PyTorch frameworks.\n",
        "| Here we will illustrate off-the-shelf/pretrained CNN models quantization for Akida using\n",
        "  [MobileNet V2](https://huggingface.co/docs/transformers/model_doc/mobilenet_v2)_ from\n",
        "  the [Hugging Face Hub](https://huggingface.co/docs/hub/index)_.\n",
        "\n",
        ".. Note::\n",
        "   | Off-the-shelf CNN models refer to already trained floating point models.\n",
        "   | Their training recipe and framework have no importance as long as they can be exported\n",
        "     to [ONNX](https://onnx.ai)_.\n",
        "   | Note however that this pathway offers slightly less flexibility than our default,\n",
        "     TensorFlow-based pathway - specifically, fine tuning of the quantized model is\n",
        "     not possible.\n",
        "   | In most cases, that won't matter, there should be almost no performance drop when\n",
        "     quantizing to 8-bit anyway.\n",
        "\n",
        ".. Note::\n",
        "   | This tutorial leverages the [Optimum toolkit](https://huggingface.co/docs/optimum/main/en/exporters/onnx/usage_guides/export_a_model)_,\n",
        "     an external tool, based on [PyTorch](https://pytorch.org/)_, that allows models direct\n",
        "     download and export to  ONNX.\n",
        "\n",
        "```\n",
        "pip install optimum[exporters]\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Workflow overview\n",
        "\n",
        ".. figure:: ../../img/off_the_shelf_flow.png\n",
        "   :target: ../../_images/off_the_shelf_flow.png\n",
        "   :alt: Off-the-shelf models quantization flow\n",
        "   :scale: 60 %\n",
        "   :align: center\n",
        "\n",
        "   Off-the-shelf CNN models Akida workflow\n",
        "\n",
        "As shown in the figure above, the [QuantizeML toolkit](../../api_reference/quantizeml_apis.html)_ allows the Post Training Quantization of ONNX\n",
        "models.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data preparation\n",
        "\n",
        "Given that the reference model was trained on [ImageNet](https://www.image-net.org/)_ dataset\n",
        "(which is not publicly available), this tutorial used a subset of 10 copyright free images.\n",
        "See [data preparation](../general/plot_1_akidanet_imagenet.html#dataset-preparation)_\n",
        "for more details.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import csv\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow.io import read_file\n",
        "from tensorflow.image import decode_jpeg\n",
        "from tensorflow.keras.utils import get_file\n",
        "\n",
        "from akida_models.imagenet import preprocessing\n",
        "\n",
        "# Model specification and hyperparameters\n",
        "NUM_CHANNELS = 3\n",
        "IMAGE_SIZE = 224\n",
        "\n",
        "num_images = 10\n",
        "\n",
        "# Retrieve dataset file from Brainchip data server\n",
        "file_path = get_file(\n",
        "    \"imagenet_like.zip\",\n",
        "    \"https://data.brainchip.com/dataset-mirror/imagenet_like/imagenet_like.zip\",\n",
        "    cache_subdir='datasets/imagenet_like',\n",
        "    extract=True)\n",
        "data_folder = os.path.dirname(file_path)\n",
        "\n",
        "# Load images for test set\n",
        "x_test_files = []\n",
        "x_test_raw = np.zeros((num_images, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS)).astype('uint8')\n",
        "for id in range(num_images):\n",
        "    test_file = 'image_' + str(id + 1).zfill(2) + '.jpg'\n",
        "    x_test_files.append(test_file)\n",
        "    img_path = os.path.join(data_folder, test_file)\n",
        "    base_image = read_file(img_path)\n",
        "    image = decode_jpeg(base_image, channels=NUM_CHANNELS)\n",
        "    image = preprocessing.preprocess_image(image, (IMAGE_SIZE, IMAGE_SIZE))\n",
        "    x_test_raw[id, :, :, :] = np.expand_dims(image, axis=0)\n",
        "\n",
        "# Parse labels file\n",
        "fname = os.path.join(data_folder, 'labels_validation.txt')\n",
        "validation_labels = dict()\n",
        "with open(fname, newline='') as csvfile:\n",
        "    reader = csv.reader(csvfile, delimiter=' ')\n",
        "    for row in reader:\n",
        "        validation_labels[row[0]] = row[1]\n",
        "\n",
        "# Get labels for the test set by index\n",
        "# Note: Hugging Face models reserve the first index to null predictions\n",
        "# (labeled as 'background' id). That is why we increase in '1' the original label id.\n",
        "labels_test = np.zeros(num_images)\n",
        "for i in range(num_images):\n",
        "    labels_test[i] = int(validation_labels[x_test_files[i]]) + 1\n",
        "\n",
        "print(f'{num_images} images loaded and preprocessed.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As illustrated in `1. Workflow overview`_, the model's source is at the user's\n",
        "discretion. Here, we know a priori that MobileNet V2 was trained with\n",
        "images normalized within [-1, 1] interval. Also, ONNX models are usually\n",
        "saved with a `channels-first` format, input images are expected to be passed\n",
        "with the channels dimension on `axis = 1`.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Project images in the range [-1, 1]\n",
        "x_test = (x_test_raw / 127.5 - 1).astype('float32')\n",
        "\n",
        "# Transpose the channels to the first axis\n",
        "x_test = np.transpose(x_test, (0, 3, 1, 2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Download and export\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1. Download ONNX MobileNet V2\n",
        "\n",
        "There are many repositories with models saved in ONNX format. In this example the\n",
        "[Optimum API](https://huggingface.co/docs/optimum/main/en/exporters/onnx/usage_guides/export_a_model)_\n",
        "is used for downloading and exporting models to ONNX.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from optimum.exporters.onnx import main_export\n",
        "\n",
        "# Download and convert MobiletNet V2 to ONNX\n",
        "main_export(model_name_or_path=\"google/mobilenet_v2_1.0_224\",\n",
        "            task=\"image-classification\",\n",
        "            output=\"./\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import onnx\n",
        "\n",
        "# Load the model in memory\n",
        "model_onnx = onnx.load_model(\"./model.onnx\")\n",
        "print(onnx.helper.printable_graph(model_onnx.graph))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2. Evaluate model performances\n",
        "\n",
        "The [ONNXRuntime](https://onnxruntime.ai)_ package is a cross-platform\n",
        "accelerator capable of loading and running models described in ONNX format.\n",
        "We take advantage of this framework to run the ONNX models and evaluate\n",
        "their performances.\n",
        "\n",
        ".. Note:: We only compute accuracy on 10 images.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from onnxruntime import InferenceSession\n",
        "\n",
        "\n",
        "def evaluate_onnx_model(model):\n",
        "    sess = InferenceSession(model.SerializeToString())\n",
        "    # Calculate outputs by running images through the session\n",
        "    outputs = sess.run(None, {model.graph.input[0].name: x_test})\n",
        "    # The class with the highest score is what we choose as prediction\n",
        "    predicted = np.squeeze(np.argmax(outputs[0], 1))\n",
        "    # Compute the model accuracy\n",
        "    accuracy = (predicted == labels_test).sum() / num_images\n",
        "    return accuracy\n",
        "\n",
        "\n",
        "accuracy_floating = evaluate_onnx_model(model_onnx)\n",
        "print(f'Floating point model accuracy: {100 * accuracy_floating:.2f} %')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Quantize\n",
        "\n",
        "| Akida processes integer activations and weights. Therefore, the floating point model\n",
        "  must be quantized in preparation to run on an Akida accelerator.\n",
        "| [QuantizeML quantize()](../../api_reference/quantizeml_apis.html#quantizeml.models.quantize)_\n",
        "  function recognizes [ModelProto](https://onnx.ai/onnx/api/classes.html#modelproto)_ objects\n",
        "  and quantizes them for Akida. The result is another ``ModelProto``, compatible with the\n",
        "  [CNN2SNN Toolkit](../../user_guide/cnn2snn.html)_.\n",
        "| The table below summarizes the obtained accuracy at the various stages using the full\n",
        "  ImageNet dataset.\n",
        "\n",
        "+-------------------------------+----------------+--------------------+----------------+\n",
        "| Calibration parameters        | Float accuracy | Quantized accuracy | Akida accuracy |\n",
        "+===============================+================+====================+================+\n",
        "| Random samples / per-tensor   | 71.790         | 70.550             | 70.588         |\n",
        "+-------------------------------+----------------+--------------------+----------------+\n",
        "| Imagenet samples / per-tensor | 71.790         | 70.472             | 70.628         |\n",
        "+-------------------------------+----------------+--------------------+----------------+\n",
        "\n",
        ".. Note::\n",
        "   Please refer to the [QuantizeML toolkit user guide](../../user_guide/quantizeml.html)_\n",
        "   and the [Advanced QuantizeML tutorial](plot_0_advanced_quantizeml.html)_ for details\n",
        "   about quantization parameters.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from quantizeml.layers import QuantizationParams\n",
        "from quantizeml.models import quantize\n",
        "\n",
        "# Quantize with activations quantized per tensor\n",
        "qparams = QuantizationParams(per_tensor_activations=True)\n",
        "model_quantized = quantize(model_onnx, qparams=qparams, num_samples=5)\n",
        "\n",
        "# Evaluate the quantized model performance\n",
        "accuracy = evaluate_onnx_model(model_quantized)\n",
        "print(f'Quantized model accuracy: {100 * accuracy:.2f} %')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ".. Note:: Once the model is quantized, the [convert](../../api_reference/cnn2snn_apis.html#cnn2snn.convert)_\n",
        "   function must be used to retrieve a model in Akida format ready for inference. Please\n",
        "   refer to the [PyTorch to Akida workflow](../general/plot_8_global_pytorch_workflow.html)_ for further details.\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

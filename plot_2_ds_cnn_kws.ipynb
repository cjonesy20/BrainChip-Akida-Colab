{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cjonesy20/BrainChip-Akida-Colab/blob/main/plot_2_ds_cnn_kws.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://doc.brainchipinc.com/_downloads/0792bc3dc7b01941f86b4f993c20ab5f/requirements.txt\n",
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6hSoSKuT1fKN",
        "outputId": "780b9472-0c00-49a8-c15f-20204b839f10"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-10-09 19:07:22--  https://doc.brainchipinc.com/_downloads/0792bc3dc7b01941f86b4f993c20ab5f/requirements.txt\n",
            "Resolving doc.brainchipinc.com (doc.brainchipinc.com)... 185.199.108.153, 185.199.109.153, 185.199.110.153, ...\n",
            "Connecting to doc.brainchipinc.com (doc.brainchipinc.com)|185.199.108.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 146 [text/plain]\n",
            "Saving to: ‘requirements.txt’\n",
            "\n",
            "requirements.txt    100%[===================>]     146  --.-KB/s    in 0s      \n",
            "\n",
            "2023-10-09 19:07:22 (5.72 MB/s) - ‘requirements.txt’ saved [146/146]\n",
            "\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (3.7.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (1.2.2)\n",
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (4.9.3)\n",
            "Collecting akida~=2.4.0 (from -r requirements.txt (line 7))\n",
            "  Downloading akida-2.4.0-cp310-cp310-manylinux_2_28_x86_64.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cnn2snn~=2.4.0 (from -r requirements.txt (line 8))\n",
            "  Downloading cnn2snn-2.4.0.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.2/67.2 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting akida_models~=1.2.0 (from -r requirements.txt (line 9))\n",
            "  Downloading akida_models-1.2.0.tar.gz (124 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.6/124.6 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 2)) (1.1.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 2)) (0.12.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 2)) (4.43.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 2)) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 2)) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 2)) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 2)) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 2)) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 2)) (2.8.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r requirements.txt (line 3)) (1.11.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r requirements.txt (line 3)) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r requirements.txt (line 3)) (3.2.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->-r requirements.txt (line 4)) (1.4.0)\n",
            "Requirement already satisfied: array-record in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->-r requirements.txt (line 4)) (0.4.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->-r requirements.txt (line 4)) (8.1.7)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->-r requirements.txt (line 4)) (0.1.8)\n",
            "Requirement already satisfied: etils[enp,epath,etree]>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->-r requirements.txt (line 4)) (1.5.0)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->-r requirements.txt (line 4)) (2.3)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->-r requirements.txt (line 4)) (3.20.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->-r requirements.txt (line 4)) (5.9.5)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->-r requirements.txt (line 4)) (2.31.0)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->-r requirements.txt (line 4)) (1.14.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->-r requirements.txt (line 4)) (2.3.0)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->-r requirements.txt (line 4)) (0.10.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->-r requirements.txt (line 4)) (4.66.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->-r requirements.txt (line 4)) (1.15.0)\n",
            "Collecting tensorflow<2.13.0,>=2.10.0 (from cnn2snn~=2.4.0->-r requirements.txt (line 8))\n",
            "  Downloading tensorflow-2.12.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (585.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m585.9/585.9 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting keras<2.13.0,>=2.10.0 (from cnn2snn~=2.4.0->-r requirements.txt (line 8))\n",
            "  Downloading keras-2.12.0-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m64.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting quantizeml~=0.5.0 (from cnn2snn~=2.4.0->-r requirements.txt (line 8))\n",
            "  Downloading quantizeml-0.5.3.tar.gz (70 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.7/70.7 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from akida_models~=1.2.0->-r requirements.txt (line 9)) (4.8.0.76)\n",
            "Collecting mtcnn (from akida_models~=1.2.0->-r requirements.txt (line 9))\n",
            "  Downloading mtcnn-0.1.1-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m70.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: imgaug in /usr/local/lib/python3.10/dist-packages (from akida_models~=1.2.0->-r requirements.txt (line 9)) (0.4.0)\n",
            "Collecting trimesh (from akida_models~=1.2.0->-r requirements.txt (line 9))\n",
            "  Downloading trimesh-3.23.5-py3-none-any.whl (685 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m685.4/685.4 kB\u001b[0m \u001b[31m45.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-addons>=0.18.0 (from akida_models~=1.2.0->-r requirements.txt (line 9))\n",
            "  Downloading tensorflow_addons-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (612 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m612.1/612.1 kB\u001b[0m \u001b[31m41.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets->-r requirements.txt (line 4)) (2023.6.0)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets->-r requirements.txt (line 4)) (6.1.0)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets->-r requirements.txt (line 4)) (4.5.0)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets->-r requirements.txt (line 4)) (3.17.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->-r requirements.txt (line 2)) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow-datasets->-r requirements.txt (line 4)) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow-datasets->-r requirements.txt (line 4)) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow-datasets->-r requirements.txt (line 4)) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow-datasets->-r requirements.txt (line 4)) (2023.7.22)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13.0,>=2.10.0->cnn2snn~=2.4.0->-r requirements.txt (line 8)) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13.0,>=2.10.0->cnn2snn~=2.4.0->-r requirements.txt (line 8)) (23.5.26)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13.0,>=2.10.0->cnn2snn~=2.4.0->-r requirements.txt (line 8)) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13.0,>=2.10.0->cnn2snn~=2.4.0->-r requirements.txt (line 8)) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13.0,>=2.10.0->cnn2snn~=2.4.0->-r requirements.txt (line 8)) (1.59.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13.0,>=2.10.0->cnn2snn~=2.4.0->-r requirements.txt (line 8)) (3.9.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13.0,>=2.10.0->cnn2snn~=2.4.0->-r requirements.txt (line 8)) (0.4.16)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13.0,>=2.10.0->cnn2snn~=2.4.0->-r requirements.txt (line 8)) (16.0.6)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13.0,>=2.10.0->cnn2snn~=2.4.0->-r requirements.txt (line 8)) (3.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13.0,>=2.10.0->cnn2snn~=2.4.0->-r requirements.txt (line 8)) (67.7.2)\n",
            "Collecting tensorboard<2.13,>=2.12 (from tensorflow<2.13.0,>=2.10.0->cnn2snn~=2.4.0->-r requirements.txt (line 8))\n",
            "  Downloading tensorboard-2.12.3-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m70.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-estimator<2.13,>=2.12.0 (from tensorflow<2.13.0,>=2.10.0->cnn2snn~=2.4.0->-r requirements.txt (line 8))\n",
            "  Downloading tensorflow_estimator-2.12.0-py2.py3-none-any.whl (440 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.7/440.7 kB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting wrapt (from tensorflow-datasets->-r requirements.txt (line 4))\n",
            "  Downloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13.0,>=2.10.0->cnn2snn~=2.4.0->-r requirements.txt (line 8)) (0.34.0)\n",
            "Collecting typeguard<3.0.0,>=2.7 (from tensorflow-addons>=0.18.0->akida_models~=1.2.0->-r requirements.txt (line 9))\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: scikit-image>=0.14.2 in /usr/local/lib/python3.10/dist-packages (from imgaug->akida_models~=1.2.0->-r requirements.txt (line 9)) (0.19.3)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.10/dist-packages (from imgaug->akida_models~=1.2.0->-r requirements.txt (line 9)) (2.31.5)\n",
            "Requirement already satisfied: Shapely in /usr/local/lib/python3.10/dist-packages (from imgaug->akida_models~=1.2.0->-r requirements.txt (line 9)) (2.0.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-metadata->tensorflow-datasets->-r requirements.txt (line 4)) (1.60.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<2.13.0,>=2.10.0->cnn2snn~=2.4.0->-r requirements.txt (line 8)) (0.41.2)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow<2.13.0,>=2.10.0->cnn2snn~=2.4.0->-r requirements.txt (line 8)) (0.3.1)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.14.2->imgaug->akida_models~=1.2.0->-r requirements.txt (line 9)) (3.1)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.14.2->imgaug->akida_models~=1.2.0->-r requirements.txt (line 9)) (2023.9.26)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.14.2->imgaug->akida_models~=1.2.0->-r requirements.txt (line 9)) (1.4.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13.0,>=2.10.0->cnn2snn~=2.4.0->-r requirements.txt (line 8)) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13.0,>=2.10.0->cnn2snn~=2.4.0->-r requirements.txt (line 8)) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13.0,>=2.10.0->cnn2snn~=2.4.0->-r requirements.txt (line 8)) (3.4.4)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13.0,>=2.10.0->cnn2snn~=2.4.0->-r requirements.txt (line 8)) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13.0,>=2.10.0->cnn2snn~=2.4.0->-r requirements.txt (line 8)) (3.0.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<2.13.0,>=2.10.0->cnn2snn~=2.4.0->-r requirements.txt (line 8)) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<2.13.0,>=2.10.0->cnn2snn~=2.4.0->-r requirements.txt (line 8)) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<2.13.0,>=2.10.0->cnn2snn~=2.4.0->-r requirements.txt (line 8)) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow<2.13.0,>=2.10.0->cnn2snn~=2.4.0->-r requirements.txt (line 8)) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow<2.13.0,>=2.10.0->cnn2snn~=2.4.0->-r requirements.txt (line 8)) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<2.13.0,>=2.10.0->cnn2snn~=2.4.0->-r requirements.txt (line 8)) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow<2.13.0,>=2.10.0->cnn2snn~=2.4.0->-r requirements.txt (line 8)) (3.2.2)\n",
            "Building wheels for collected packages: cnn2snn, akida_models, quantizeml\n",
            "  Building wheel for cnn2snn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for cnn2snn: filename=cnn2snn-2.4.0-py3-none-any.whl size=114399 sha256=157221be5a6701453add7db9b61d24f2155e5f086a9d41432dcbd41a727a6d56\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/98/ad/e4e548669cc179725f064bea32d236178cf78a7988afd7c8d3\n",
            "  Building wheel for akida_models (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for akida_models: filename=akida_models-1.2.0-py3-none-any.whl size=172972 sha256=572f3d955bbb36e782c774ede9f1f5a625282bd45813e75dc98e6bd348ce1bcf\n",
            "  Stored in directory: /root/.cache/pip/wheels/76/06/dc/4deba7159c4fc9c0eeeede19de8190fd7a36c4a13ec8061a0d\n",
            "  Building wheel for quantizeml (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for quantizeml: filename=quantizeml-0.5.3-py3-none-any.whl size=116161 sha256=375d5df4d465692393fdc6725b5b6ac1bc36eb9f1e01ab63453366201624630e\n",
            "  Stored in directory: /root/.cache/pip/wheels/81/88/61/cddeb1338ef56af4e354e9e038b329276c45273425ab2ac51b\n",
            "Successfully built cnn2snn akida_models quantizeml\n",
            "Installing collected packages: wrapt, typeguard, trimesh, tensorflow-estimator, keras, akida, tensorflow-addons, mtcnn, tensorboard, tensorflow, quantizeml, cnn2snn, akida_models\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.15.0\n",
            "    Uninstalling wrapt-1.15.0:\n",
            "      Successfully uninstalled wrapt-1.15.0\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.13.0\n",
            "    Uninstalling tensorflow-estimator-2.13.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.13.0\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.13.1\n",
            "    Uninstalling keras-2.13.1:\n",
            "      Successfully uninstalled keras-2.13.1\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.13.0\n",
            "    Uninstalling tensorboard-2.13.0:\n",
            "      Successfully uninstalled tensorboard-2.13.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.13.0\n",
            "    Uninstalling tensorflow-2.13.0:\n",
            "      Successfully uninstalled tensorflow-2.13.0\n",
            "Successfully installed akida-2.4.0 akida_models-1.2.0 cnn2snn-2.4.0 keras-2.12.0 mtcnn-0.1.1 quantizeml-0.5.3 tensorboard-2.12.3 tensorflow-2.12.1 tensorflow-addons-0.21.0 tensorflow-estimator-2.12.0 trimesh-3.23.5 typeguard-2.13.3 wrapt-1.14.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ZFCXpqXo0qO4"
      },
      "outputs": [],
      "source": [
        "%matplotlib notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLzGWl060qO5"
      },
      "source": [
        "\n",
        "# DS-CNN/KWS inference\n",
        "\n",
        "This tutorial illustrates how to build a basic speech recognition\n",
        "Akida network that recognizes thirty-two different words.\n",
        "\n",
        "The model will be first defined as a CNN and trained in Keras, then\n",
        "converted using the [CNN2SNN toolkit](../../user_guide/cnn2snn.html)_.\n",
        "\n",
        "This example uses a Keyword Spotting Dataset prepared using\n",
        "**TensorFlow** [audio recognition\n",
        "example](https://www.tensorflow.org/tutorials/audio/simple_audio)_ utils.\n",
        "\n",
        "The words to recognize are first converted to [spectrogram\n",
        "images](https://github.com/tensorflow/docs/blob/master/site/en/r1/tutorials/sequences/audio_recognition.md#how-does-this-model-work)_\n",
        "that allows us to use a model architecture that is typically used for\n",
        "image recognition tasks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4yEFw0Tb0qO7"
      },
      "source": [
        "## 1. Load the preprocessed dataset\n",
        "\n",
        "The TensorFlow [speech_commands](http://download.tensorflow.org/data/speech_commands_v0.02.tar.gz)_\n",
        "dataset is used for training and validation. All keywords except \"backward\",\n",
        "\"follow\" and \"forward\", are retrieved. These three words are kept to\n",
        "illustrate the edge learning in this\n",
        "[edge example](../edge/plot_1_edge_learning_kws.html)_.\n",
        "The data are not directly used for training. They are preprocessed,\n",
        "transforming the audio files into MFCC features, well-suited for CNN networks.\n",
        "A pickle file containing the preprocessed data is available on our data\n",
        "server.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s1i8OFYo0qO8",
        "outputId": "03872136-f04f-4c65-fff4-10d77aef7edf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from http://data.brainchip.com/dataset-mirror/kws/kws_preprocessed_all_words_except_backward_follow_forward.pkl\n",
            "62628765/62628765 [==============================] - 3s 0us/step\n",
            "Wanted words and labels:\n",
            " {'six': 23, 'three': 25, 'seven': 21, 'bed': 1, 'eight': 6, 'yes': 31, 'cat': 3, 'on': 18, 'one': 19, 'stop': 24, 'two': 27, 'house': 11, 'five': 7, 'down': 5, 'four': 8, 'go': 9, 'up': 28, 'learn': 12, 'no': 16, 'bird': 2, 'zero': 32, 'nine': 15, 'visual': 29, 'wow': 30, 'sheila': 22, 'marvin': 14, 'off': 17, 'right': 20, 'left': 13, 'happy': 10, 'dog': 4, 'tree': 26, '_silence_': 0}\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "\n",
        "from tensorflow.keras.utils import get_file\n",
        "\n",
        "# Fetch pre-processed data for 32 keywords\n",
        "fname = get_file(\n",
        "    fname='kws_preprocessed_all_words_except_backward_follow_forward.pkl',\n",
        "    origin=\"http://data.brainchip.com/dataset-mirror/kws/kws_preprocessed_all_words_except_backward_follow_forward.pkl\",\n",
        "    cache_subdir='datasets/kws')\n",
        "with open(fname, 'rb') as f:\n",
        "    [_, _, x_valid, y_valid, _, _, word_to_index, _] = pickle.load(f)\n",
        "\n",
        "# Preprocessed dataset parameters\n",
        "num_classes = len(word_to_index)\n",
        "\n",
        "print(\"Wanted words and labels:\\n\", word_to_index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tPML5nU0qO8"
      },
      "source": [
        "## 2. Load a pre-trained native Keras model\n",
        "\n",
        "The model consists of:\n",
        "\n",
        "* a first convolutional layer accepting dense inputs (images),\n",
        "* several separable convolutional layers preserving spatial dimensions,\n",
        "* a global pooling reducing the spatial dimensions to a single pixel,\n",
        "* a last separable convolutional to reduce the number of outputs\n",
        "* a final fully connected layer to classify words\n",
        "\n",
        "All layers are followed by a batch normalization and a ReLU activation.\n",
        "\n",
        "This model was obtained with unconstrained float weights and activations after\n",
        "16 epochs of training.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qPyV0yqS0qO8"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Retrieve the model file from the BrainChip data server\n",
        "model_file = get_file(\"ds_cnn_kws.h5\",\n",
        "                      \"http://data.brainchip.com/models/ds_cnn/ds_cnn_kws.h5\",\n",
        "                      cache_subdir='models')\n",
        "\n",
        "# Load the native Keras pre-trained model\n",
        "model_keras = load_model(model_file)\n",
        "model_keras.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ogfrgvNs0qO8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Check Keras Model performance\n",
        "potentials_keras = model_keras.predict(x_valid)\n",
        "preds_keras = np.squeeze(np.argmax(potentials_keras, 1))\n",
        "\n",
        "accuracy = accuracy_score(y_valid, preds_keras)\n",
        "print(\"Accuracy: \" + \"{0:.2f}\".format(100 * accuracy) + \"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1RP9XfVc0qO9"
      },
      "source": [
        "## 3. Load a pre-trained quantized Keras model satisfying Akida NSoC requirements\n",
        "\n",
        "The above native Keras model is quantized and fine-tuned to get a quantized\n",
        "Keras model satisfying the Akida NSoC requirements.\n",
        "The first convolutional layer uses 8 bits weights, but other layers use\n",
        "4 bits weights.\n",
        "\n",
        "All activations are 4 bits except for the final Separable Convolutional that\n",
        "uses binary activations.\n",
        "\n",
        "Pre-trained weights were obtained after a few training episodes:\n",
        "\n",
        "* we train the model with quantized activations only, with weights initialized\n",
        "  from those trained in the previous episode (native Keras model),\n",
        "* then, we train the model with quantized weights, with both weights and\n",
        "  activations initialized from those trained in the previous episode,\n",
        "* finally, we train the model with quantized weights and activations and by\n",
        "  gradually increasing quantization in the last layer.\n",
        "\n",
        "The table below summarizes the results obtained when preparing the\n",
        "weights stored under (http://data.brainchip.com/models/ds_cnn/) :\n",
        "\n",
        "+---------+----------------+----------------------------+----------+--------+\n",
        "| Episode | Weights Quant. | Activ. Quant. / last layer | Accuracy | Epochs |\n",
        "+=========+================+============================+==========+========+\n",
        "| 1       | N/A            | N/A                        | 93.06 %  | 16     |\n",
        "+---------+----------------+----------------------------+----------+--------+\n",
        "| 2       | N/A            | 4 bits / 4 bits            | 92.30 %  | 16     |\n",
        "+---------+----------------+----------------------------+----------+--------+\n",
        "| 3       | 8/4 bits       | 4 bits / 4 bits            | 92.11 %  | 16     |\n",
        "+---------+----------------+----------------------------+----------+--------+\n",
        "| 4       | 8/4 bits       | 4 bits / 3 bits            | 92.38 %  | 16     |\n",
        "+---------+----------------+----------------------------+----------+--------+\n",
        "| 5       | 8/4 bits       | 4 bits / 2 bits            | 92.23 %  | 16     |\n",
        "+---------+----------------+----------------------------+----------+--------+\n",
        "| 6       | 8/4 bits       | 4 bits / 1 bit             | 92.22 %  | 16     |\n",
        "+---------+----------------+----------------------------+----------+--------+\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lr8pSN0B0qO9"
      },
      "outputs": [],
      "source": [
        "from akida_models import ds_cnn_kws_pretrained\n",
        "\n",
        "# Load the pre-trained quantized model\n",
        "model_keras_quantized = ds_cnn_kws_pretrained()\n",
        "model_keras_quantized.summary()\n",
        "\n",
        "# Check Model performance\n",
        "potentials_keras_q = model_keras_quantized.predict(x_valid)\n",
        "preds_keras_q = np.squeeze(np.argmax(potentials_keras_q, 1))\n",
        "\n",
        "accuracy_q = accuracy_score(y_valid, preds_keras_q)\n",
        "print(\"Accuracy: \" + \"{0:.2f}\".format(100 * accuracy_q) + \"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcHKEfvs0qO9"
      },
      "source": [
        "## 4. Conversion to Akida\n",
        "\n",
        "We convert the model to Akida and then evaluate the performances on the\n",
        "dataset.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C5JxMgA10qO9"
      },
      "outputs": [],
      "source": [
        "from cnn2snn import convert\n",
        "\n",
        "# Convert the model\n",
        "model_akida = convert(model_keras_quantized)\n",
        "model_akida.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WWrm_trY0qO9"
      },
      "outputs": [],
      "source": [
        "# Check Akida model performance\n",
        "preds_akida = model_akida.predict_classes(x_valid, num_classes=num_classes)\n",
        "\n",
        "accuracy = accuracy_score(y_valid, preds_akida)\n",
        "print(\"Accuracy: \" + \"{0:.2f}\".format(100 * accuracy) + \"%\")\n",
        "\n",
        "# For non-regression purpose\n",
        "assert accuracy > 0.9"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aedpvkre0qO-"
      },
      "source": [
        "## 5. Confusion matrix\n",
        "\n",
        "The confusion matrix provides a good summary of what mistakes the\n",
        "network is making.\n",
        "\n",
        "Per scikit-learn convention it displays the true class in each row (ie\n",
        "on each row you can see what the network predicted for the corresponding\n",
        "word).\n",
        "\n",
        "Please refer to the Tensorflow [audio\n",
        "recognition](https://github.com/tensorflow/docs/blob/master/site/en/r1/tutorials/sequences/audio_recognition.md#confusion-matrix)_\n",
        "example for a detailed explanation of the confusion matrix.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xHYNeE7r0qO-"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Create confusion matrix\n",
        "cm = confusion_matrix(y_valid, preds_akida,\n",
        "                      labels=list(word_to_index.values()))\n",
        "\n",
        "# Normalize\n",
        "cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "# Display confusion matrix\n",
        "plt.rcParams[\"figure.figsize\"] = (16, 16)\n",
        "plt.figure()\n",
        "\n",
        "title = 'Confusion matrix'\n",
        "cmap = plt.cm.Blues\n",
        "\n",
        "plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "plt.title(title)\n",
        "plt.colorbar()\n",
        "tick_marks = np.arange(len(word_to_index))\n",
        "plt.xticks(tick_marks, word_to_index, rotation=45)\n",
        "plt.yticks(tick_marks, word_to_index)\n",
        "\n",
        "thresh = cm.max() / 2.\n",
        "for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "    plt.text(j,\n",
        "             i,\n",
        "             format(cm[i, j], '.2f'),\n",
        "             horizontalalignment=\"center\",\n",
        "             color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "plt.ylabel('True label')\n",
        "plt.xlabel('Predicted label')\n",
        "plt.autoscale()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!wget https://doc.brainchipinc.com/_downloads/0792bc3dc7b01941f86b4f993c20ab5f/requirements.txt\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# YOLO/PASCAL-VOC detection tutorial\n",
        "\n",
        "This tutorial demonstrates that Akida can perform object detection. This is illustrated using a\n",
        "subset of the\n",
        "[PASCAL-VOC 2007 dataset](http://host.robots.ox.ac.uk/pascal/VOC/voc2007/htmldoc/index.html)_\n",
        "with \"car\" and \"person\" classes only. The YOLOv2 architecture from\n",
        "[Redmon et al (2016)](https://arxiv.org/pdf/1506.02640.pdf) has been chosen to\n",
        "tackle this object detection problem.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Introduction\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.1 Object detection\n",
        "\n",
        "Object detection is a computer vision task that combines two elemental tasks:\n",
        "\n",
        " - object classification that consists in assigning a class label to an image\n",
        "   like shown in the [AkidaNet/ImageNet inference](plot_1_akidanet_imagenet.html)\n",
        "   example\n",
        " - object localization that consists in drawing a bounding box around one or\n",
        "   several objects in an image\n",
        "\n",
        "One can learn more about the subject reading this [introduction to object\n",
        "detection blog article](https://machinelearningmastery.com/object-recognition-with-deep-learning/).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.2 YOLO key concepts\n",
        "\n",
        "You Only Look Once (YOLO) is a deep neural network architecture dedicated to\n",
        "object detection.\n",
        "\n",
        "As opposed to classic networks that handle object detection, YOLO predicts\n",
        "bounding boxes (localization task) and class probabilities (classification\n",
        "task) from a single neural network in a single evaluation. The object\n",
        "detection task is reduced to a regression problem to spatially separated boxes\n",
        "and associated class probabilities.\n",
        "\n",
        "YOLO base concept is to divide an input image into regions, forming a grid,\n",
        "and to predict bounding boxes and probabilities for each region. The bounding\n",
        "boxes are weighted by the prediction probabilities.\n",
        "\n",
        "YOLO also uses the concept of \"anchors boxes\" or \"prior boxes\". The network\n",
        "does not actually predict the actual bounding boxes but offsets from anchors\n",
        "boxes which are templates (width/height ratio) computed by clustering the\n",
        "dimensions of the ground truth boxes from the training dataset. The anchors\n",
        "then represent the average shape and size of the objects to detect. More\n",
        "details on the anchors boxes concept are given in [this blog article](https://medium.com/@andersasac/anchor-boxes-the-key-to-quality-object-detection-ddf9d612d4f9).\n",
        "\n",
        "Additional information about YOLO can be found on the [Darknet website](https://pjreddie.com/darknet/yolov2/) and source code for the preprocessing\n",
        "and postprocessing functions that are included in akida_models package (see\n",
        "the [processing section](../../api_reference/akida_models_apis.html#processing)\n",
        "in the model zoo) is largely inspired from\n",
        "[experiencor github](https://github.com/experiencor/keras-yolo2).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Preprocessing tools\n",
        "\n",
        "As this example focuses on car and person detection only, a subset of VOC has\n",
        "been prepared with test images from VOC2007 that contains at least one\n",
        "of the occurence of the two classes. Just like the VOC dataset, the subset\n",
        "contains an image folder, an annotation folder and a text file listing the\n",
        "file names of interest.\n",
        "\n",
        "The [YOLO toolkit](../../api_reference/akida_models_apis.html#yolo-toolkit)\n",
        "offers several methods to prepare data for processing, see\n",
        "[load_image](../../api_reference/akida_models_apis.html#akida_models.detection.processing.load_image),\n",
        "[preprocess_image](../../api_reference/akida_models_apis.html#akida_models.detection.processing.preprocess_image)\n",
        "or [parse_voc_annotations](../../api_reference/akida_models_apis.html#akida_models.detection.processing.parse_voc_annotations).\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "from akida_models import fetch_file\n",
        "from akida_models.detection.processing import parse_voc_annotations\n",
        "\n",
        "# Download validation set from Brainchip data server\n",
        "data_path = fetch_file(\n",
        "    fname=\"voc_test_car_person.tar.gz\",\n",
        "    origin=\"https://data.brainchip.com/dataset-mirror/voc/voc_test_car_person.tar.gz\",\n",
        "    cache_subdir='datasets/voc',\n",
        "    extract=True)\n",
        "\n",
        "data_dir = os.path.dirname(data_path)\n",
        "gt_folder = os.path.join(data_dir, 'voc_test_car_person', 'Annotations')\n",
        "image_folder = os.path.join(data_dir, 'voc_test_car_person', 'JPEGImages')\n",
        "file_path = os.path.join(\n",
        "    data_dir, 'voc_test_car_person', 'test_car_person.txt')\n",
        "labels = ['car', 'person']\n",
        "\n",
        "val_data = parse_voc_annotations(gt_folder, image_folder, file_path, labels)\n",
        "print(\"Loaded VOC2007 test data for car and person classes: \"\n",
        "      f\"{len(val_data)} images.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Anchors can also be computed easily using YOLO toolkit.\n",
        "\n",
        ".. Note:: The following code is given as an example. In a real use case\n",
        "          scenario, anchors are computed on the training dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from akida_models.detection.generate_anchors import generate_anchors\n",
        "\n",
        "num_anchors = 5\n",
        "grid_size = (7, 7)\n",
        "anchors_example = generate_anchors(val_data, num_anchors, grid_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Model architecture\n",
        "\n",
        "The [model zoo](../../api_reference/akida_models_apis.html#yolo) contains a\n",
        "YOLO model that is built upon the [AkidaNet architecture](../../api_reference/akida_models_apis.html#akida_models.akidanet_imagenet)\n",
        "and 3 separable convolutional layers at the top for bounding box and class\n",
        "estimation followed by a final separable convolutional which is the detection\n",
        "layer. Note that for efficiency, the alpha parameter in AkidaNet (network\n",
        "width or number of filter in each layer) is set to 0.5.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from akida_models import yolo_base\n",
        "\n",
        "# Create a yolo model for 2 classes with 5 anchors and grid size of 7\n",
        "classes = 2\n",
        "\n",
        "model = yolo_base(input_shape=(224, 224, 3),\n",
        "                  classes=classes,\n",
        "                  nb_box=num_anchors,\n",
        "                  alpha=0.5)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The model output can be reshaped to a more natural shape of:\n",
        "\n",
        " (grid_height, grid_width, anchors_box, 4 + 1 + num_classes)\n",
        "\n",
        "where the \"4 + 1\" term represents the coordinates of the estimated bounding\n",
        "boxes (top left x, top left y, width and height) and a confidence score. In\n",
        "other words, the output channels are actually grouped by anchor boxes, and in\n",
        "each group one channel provides either a coordinate, a global confidence score\n",
        "or a class confidence score. This process is done automatically in the\n",
        "[decode_output](../../api_reference/akida_models_apis.html#akida_models.detection.processing.decode_output)_\n",
        "function.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Reshape\n",
        "\n",
        "# Define a reshape output to be added to the YOLO model\n",
        "output = Reshape((grid_size[1], grid_size[0], num_anchors, 4 + 1 + classes),\n",
        "                 name=\"YOLO_output\")(model.output)\n",
        "\n",
        "# Build the complete model\n",
        "full_model = Model(model.input, output)\n",
        "full_model.output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Training\n",
        "\n",
        "As the YOLO model relies on Brainchip AkidaNet/ImageNet network, it is\n",
        "possible to perform transfer learning from ImageNet pretrained weights when\n",
        "training a YOLO model. See the [PlantVillage transfer learning example](plot_4_transfer_learning.html) for a detail explanation on transfer\n",
        "learning principles.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Performance\n",
        "\n",
        "The model zoo also contains an [helper method](../../api_reference/akida_models_apis.html#akida_models.yolo_voc_pretrained)\n",
        "that allows to create a YOLO model for VOC and load pretrained weights for the\n",
        "car and person detection task and the corresponding anchors. The anchors are\n",
        "used to interpret the model outputs.\n",
        "\n",
        "The metric used to evaluate YOLO is the mean average precision (mAP) which is\n",
        "the percentage of correct prediction and is given for an intersection over\n",
        "union (IoU) ratio. Scores in this example are given for the standard IoU of\n",
        "0.5 meaning that a detection is considered valid if the intersection over\n",
        "union ratio with its ground truth equivalent is above 0.5.\n",
        "\n",
        " .. Note:: A call to [evaluate_map](../../api_reference/akida_models_apis.html#akida_models.detection.map_evaluation.MapEvaluation.evaluate_map)\n",
        "           will preprocess the images, make the call to ``Model.predict`` and\n",
        "           use [decode_output](../../api_reference/akida_models_apis.html#akida_models.detection.processing.decode_output)_\n",
        "           before computing precision for all classes.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from timeit import default_timer as timer\n",
        "from akida_models import yolo_voc_pretrained\n",
        "from akida_models.detection.map_evaluation import MapEvaluation\n",
        "\n",
        "# Load the pretrained model along with anchors\n",
        "model_keras, anchors = yolo_voc_pretrained()\n",
        "model_keras.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Define the final reshape and build the model\n",
        "output = Reshape((grid_size[1], grid_size[0], num_anchors, 4 + 1 + classes),\n",
        "                 name=\"YOLO_output\")(model_keras.output)\n",
        "model_keras = Model(model_keras.input, output)\n",
        "\n",
        "# Create the mAP evaluator object\n",
        "num_images = 100\n",
        "\n",
        "map_evaluator = MapEvaluation(model_keras, val_data[:num_images], labels,\n",
        "                              anchors)\n",
        "\n",
        "# Compute the scores for all validation images\n",
        "start = timer()\n",
        "mAP, average_precisions = map_evaluator.evaluate_map()\n",
        "end = timer()\n",
        "\n",
        "for label, average_precision in average_precisions.items():\n",
        "    print(labels[label], '{:.4f}'.format(average_precision))\n",
        "print('mAP: {:.4f}'.format(mAP))\n",
        "print(f'Keras inference on {num_images} images took {end-start:.2f} s.\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Conversion to Akida\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.1 Convert to Akida model\n",
        "The last YOLO_output layer that was added for splitting channels into values\n",
        "for each box must be removed before Akida conversion.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Rebuild a model without the last layer\n",
        "compatible_model = Model(model_keras.input, model_keras.layers[-2].output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When converting to an Akida model, we just need to pass the Keras model\n",
        "to [cnn2snn.convert](../../api_reference/cnn2snn_apis.html#convert).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from cnn2snn import convert\n",
        "\n",
        "model_akida = convert(compatible_model)\n",
        "model_akida.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.1 Check performance\n",
        "\n",
        "Akida model accuracy is tested on the first *n* images of the validation set.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Create the mAP evaluator object\n",
        "map_evaluator_ak = MapEvaluation(model_akida,\n",
        "                                 val_data[:num_images],\n",
        "                                 labels,\n",
        "                                 anchors,\n",
        "                                 is_keras_model=False)\n",
        "\n",
        "# Compute the scores for all validation images\n",
        "start = timer()\n",
        "mAP_ak, average_precisions_ak = map_evaluator_ak.evaluate_map()\n",
        "end = timer()\n",
        "\n",
        "for label, average_precision in average_precisions_ak.items():\n",
        "    print(labels[label], '{:.4f}'.format(average_precision))\n",
        "print('mAP: {:.4f}'.format(mAP_ak))\n",
        "print(f'Akida inference on {num_images} images took {end-start:.2f} s.\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.2 Show predictions for a random image\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "from akida_models.detection.processing import load_image, preprocess_image, decode_output\n",
        "\n",
        "# Take a random test image\n",
        "i = np.random.randint(len(val_data))\n",
        "\n",
        "input_shape = model_akida.layers[0].input_dims\n",
        "\n",
        "# Load the image\n",
        "raw_image = load_image(val_data[i]['image_path'])\n",
        "\n",
        "# Keep the original image size for later bounding boxes rescaling\n",
        "raw_height, raw_width, _ = raw_image.shape\n",
        "\n",
        "# Pre-process the image\n",
        "image = preprocess_image(raw_image, input_shape)\n",
        "input_image = image[np.newaxis, :].astype(np.uint8)\n",
        "\n",
        "# Call evaluate on the image\n",
        "pots = model_akida.predict(input_image)[0]\n",
        "\n",
        "# Reshape the potentials to prepare for decoding\n",
        "h, w, c = pots.shape\n",
        "pots = pots.reshape((h, w, len(anchors), 4 + 1 + len(labels)))\n",
        "\n",
        "# Decode potentials into bounding boxes\n",
        "raw_boxes = decode_output(pots, anchors, len(labels))\n",
        "\n",
        "# Rescale boxes to the original image size\n",
        "pred_boxes = np.array([[\n",
        "    box.x1 * raw_width, box.y1 * raw_height, box.x2 * raw_width,\n",
        "    box.y2 * raw_height,\n",
        "    box.get_label(),\n",
        "    box.get_score()\n",
        "] for box in raw_boxes])\n",
        "\n",
        "fig = plt.figure(num='VOC2012 car and person detection by Akida')\n",
        "ax = fig.subplots(1)\n",
        "img_plot = ax.imshow(np.zeros(raw_image.shape, dtype=np.uint8))\n",
        "img_plot.set_data(raw_image)\n",
        "\n",
        "for box in pred_boxes:\n",
        "    rect = patches.Rectangle((box[0], box[1]),\n",
        "                             box[2] - box[0],\n",
        "                             box[3] - box[1],\n",
        "                             linewidth=1,\n",
        "                             edgecolor='r',\n",
        "                             facecolor='none')\n",
        "    ax.add_patch(rect)\n",
        "    class_score = ax.text(box[0],\n",
        "                          box[1] - 5,\n",
        "                          f\"{labels[int(box[4])]} - {box[5]:.2f}\",\n",
        "                          color='red')\n",
        "\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

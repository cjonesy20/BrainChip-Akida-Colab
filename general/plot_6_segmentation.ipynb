{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!wget https://doc.brainchipinc.com/_downloads/0792bc3dc7b01941f86b4f993c20ab5f/requirements.txt\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# Segmentation tutorial\n",
        "\n",
        "This example demonstrates image segmentation with an Akida-compatible model as\n",
        "illustrated through person segmentation using the [Portrait128 dataset](https://github.com/anilsathyan7/Portrait-Segmentation)_.\n",
        "\n",
        "Using pre-trained models for quick runtime, this example shows the evolution of\n",
        "model performance for a trained keras floating point model, a keras quantized and\n",
        "Quantization Aware Trained (QAT) model, and an Akida-converted model. Notice that\n",
        "the performance of the original keras floating point model is maintained throughout\n",
        "the model conversion flow.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load the dataset\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from akida_models import fetch_file\n",
        "\n",
        "# Download validation set from Brainchip data server, it contains 10% of the original dataset\n",
        "data_path = fetch_file(fname=\"val.tar.gz\",\n",
        "                       origin=\"https://data.brainchip.com/dataset-mirror/portrait128/val.tar.gz\",\n",
        "                       cache_subdir=os.path.join(\"datasets\", \"portrait128\"),\n",
        "                       extract=True)\n",
        "\n",
        "data_dir = os.path.join(os.path.dirname(data_path), \"val\")\n",
        "x_val = np.load(os.path.join(data_dir, \"val_img.npy\"))\n",
        "y_val = np.load(os.path.join(data_dir, \"val_msk.npy\")).astype('uint8')\n",
        "batch_size = 32\n",
        "steps = x_val.shape[0] // 32\n",
        "\n",
        "# Visualize some data\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "id = np.random.randint(0, x_val.shape[0])\n",
        "\n",
        "fig, axs = plt.subplots(3, 3, constrained_layout=True)\n",
        "for col in range(3):\n",
        "    axs[0, col].imshow(x_val[id + col] / 255.)\n",
        "    axs[0, col].axis('off')\n",
        "    axs[1, col].imshow(1 - y_val[id + col], cmap='Greys')\n",
        "    axs[1, col].axis('off')\n",
        "    axs[2, col].imshow(x_val[id + col] / 255. * y_val[id + col])\n",
        "    axs[2, col].axis('off')\n",
        "\n",
        "fig.suptitle('Image, mask and masked image', fontsize=10)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load a pre-trained native Keras model\n",
        "\n",
        "The model used in this example is AkidaUNet. It has an AkidaNet (0.5) backbone to extract\n",
        "features combined with a succession of [separable transposed convolutional](../../api_reference/akida_models_apis.html#akida_models.layer_blocks.sepconv_transpose_block)_\n",
        "blocks to build an image segmentation map. A pre-trained floating point keras model is\n",
        "downloaded to save training time.\n",
        "\n",
        "<div class=\"alert alert-info\"><h4>Note</h4><p>- The \"transposed\" convolutional feature is new in Akida 2.0.\n",
        "  - The \"separable transposed\" operation is realized through the combination of a QuantizeML custom\n",
        "    [DepthwiseConv2DTranspose](../../api_reference/quantizeml_apis.html#quantizeml.layers.DepthwiseConv2DTranspose)_ layer\n",
        "    with a standard pointwise convolution.</p></div>\n",
        "\n",
        "The performance of the model is evaluated using both pixel accuracy and [Binary IoU](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/BinaryIoU)_. The pixel\n",
        "accuracy describes how well the model can predict the segmentation mask pixel by pixel\n",
        "and the Binary IoU takes into account how close the predicted mask is to the ground truth.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from akida_models.model_io import load_model\n",
        "\n",
        "# Retrieve the model file from Brainchip data server\n",
        "model_file = fetch_file(fname=\"akida_unet_portrait128.h5\",\n",
        "                        origin=\"https://data.brainchip.com/models/AkidaV2/akida_unet/akida_unet_portrait128.h5\",\n",
        "                        cache_subdir='models')\n",
        "\n",
        "# Load the native Keras pre-trained model\n",
        "model_keras = load_model(model_file)\n",
        "model_keras.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from keras.metrics import BinaryIoU\n",
        "\n",
        "# Compile the native Keras model (required to evaluate the metrics)\n",
        "model_keras.compile(loss='binary_crossentropy', metrics=[BinaryIoU(), 'accuracy'])\n",
        "\n",
        "# Check Keras model performance\n",
        "_, biou, acc = model_keras.evaluate(x_val, y_val, steps=steps, verbose=0)\n",
        "\n",
        "print(f\"Keras binary IoU / pixel accuracy: {biou:.4f} / {100*acc:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Load a pre-trained quantized Keras model\n",
        "\n",
        "The next step is to quantize and potentially perform Quantize Aware Training (QAT) on the\n",
        "Keras model from the previous step. After the Keras model is quantized to 8-bits for\n",
        "all weights and activations, QAT is used to maintain the performance of the quantized\n",
        "model. Again, a pre-trained model is downloaded to save runtime.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from akida_models import akida_unet_portrait128_pretrained\n",
        "\n",
        "# Load the pre-trained quantized model\n",
        "model_quantized_keras = akida_unet_portrait128_pretrained()\n",
        "model_quantized_keras.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Compile the quantized Keras model (required to evaluate the metrics)\n",
        "model_quantized_keras.compile(loss='binary_crossentropy', metrics=[BinaryIoU(), 'accuracy'])\n",
        "\n",
        "# Check Keras model performance\n",
        "_, biou, acc = model_quantized_keras.evaluate(x_val, y_val, steps=steps, verbose=0)\n",
        "\n",
        "print(f\"Keras quantized binary IoU / pixel accuracy: {biou:.4f} / {100*acc:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Conversion to Akida\n",
        "\n",
        "Finally, the quantized Keras model from the previous step is converted into an Akida\n",
        "model and its performance is evaluated. Note that the original performance of the keras\n",
        "floating point model is maintained throughout the conversion process in this example.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from cnn2snn import convert\n",
        "\n",
        "# Convert the model\n",
        "model_akida = convert(model_quantized_keras)\n",
        "model_akida.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Check Akida model performance\n",
        "labels, pots = None, None\n",
        "\n",
        "for s in range(steps):\n",
        "    batch = x_val[s * batch_size: (s + 1) * batch_size, :]\n",
        "    label_batch = y_val[s * batch_size: (s + 1) * batch_size, :]\n",
        "    pots_batch = model_akida.predict(batch.astype('uint8'))\n",
        "\n",
        "    if labels is None:\n",
        "        labels = label_batch\n",
        "        pots = pots_batch\n",
        "    else:\n",
        "        labels = np.concatenate((labels, label_batch))\n",
        "        pots = np.concatenate((pots, pots_batch))\n",
        "preds = tf.keras.activations.sigmoid(pots)\n",
        "\n",
        "m_binary_iou = tf.keras.metrics.BinaryIoU(target_class_ids=[0, 1], threshold=0.5)\n",
        "m_binary_iou.update_state(labels, preds)\n",
        "binary_iou = m_binary_iou.result().numpy()\n",
        "\n",
        "m_accuracy = tf.keras.metrics.Accuracy()\n",
        "m_accuracy.update_state(labels, preds > 0.5)\n",
        "accuracy = m_accuracy.result().numpy()\n",
        "print(f\"Akida binary IoU / pixel accuracy: {binary_iou:.4f} / {100*accuracy:.2f}%\")\n",
        "\n",
        "# For non-regression purpose\n",
        "assert binary_iou > 0.9"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Segment a single image\n",
        "\n",
        "For visualization of the person segmentation performed by the Akida model, display a\n",
        "single image along with the segmentation produced by the original floating point model\n",
        "and the ground truth segmentation.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Estimate age on a random single image and display Keras and Akida outputs\n",
        "sample = np.expand_dims(x_val[id, :], 0)\n",
        "keras_out = model_keras(sample)\n",
        "akida_out = tf.keras.activations.sigmoid(model_akida.forward(sample.astype('uint8')))\n",
        "\n",
        "fig, axs = plt.subplots(1, 3, constrained_layout=True)\n",
        "axs[0].imshow(keras_out[0] * sample[0] / 255.)\n",
        "axs[0].set_title('Keras segmentation', fontsize=10)\n",
        "axs[0].axis('off')\n",
        "\n",
        "axs[1].imshow(akida_out[0] * sample[0] / 255.)\n",
        "axs[1].set_title('Akida segmentation', fontsize=10)\n",
        "axs[1].axis('off')\n",
        "\n",
        "axs[2].imshow(y_val[id] * sample[0] / 255.)\n",
        "axs[2].set_title('Expected segmentation', fontsize=10)\n",
        "axs[2].axis('off')\n",
        "\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!wget https://doc.brainchipinc.com/_downloads/0792bc3dc7b01941f86b4f993c20ab5f/requirements.txt\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# DS-CNN/KWS inference\n",
        "\n",
        "This tutorial illustrates the process of developing an Akida-compatible speech recognition\n",
        "model that can identify thirty-two different keywords.\n",
        "\n",
        "Initially, the model is defined as a CNN in Keras and trained regularly. Next, it undergoes\n",
        "quantization using [QuantizeML](../../user_guide/quantizeml.html)_ and finally converted\n",
        "to Akida using [CNN2SNN](../../user_guide/cnn2snn.html)_.\n",
        "\n",
        "This example uses a Keyword Spotting Dataset prepared using **TensorFlow** [audio recognition\n",
        "example](https://www.tensorflow.org/tutorials/audio/simple_audio)_ utils.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load the preprocessed dataset\n",
        "\n",
        "The TensorFlow [speech_commands](https://www.tensorflow.org/datasets/catalog/speech_commands)_\n",
        "dataset is used for training and validation. All keywords except \"backward\",\n",
        "\"follow\" and \"forward\", are retrieved. These three words are kept to\n",
        "illustrate the edge learning in this\n",
        "[edge example](../edge/plot_1_edge_learning_kws.html)_.\n",
        "\n",
        "The words to recognize have been converted to [spectrogram images](https://github.com/tensorflow/docs/blob/master/site/en/r1/tutorials/sequences/audio_recognition.md#how-does-this-model-work)_\n",
        "that allows us to use a model architecture that is typically used for image recognition tasks.\n",
        "The raw audio data have been preprocessed, transforming the audio files into MFCC features,\n",
        "well-suited for CNN networks.\n",
        "A pickle file containing the preprocessed data is available on Brainchip data server.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "from akida_models import fetch_file\n",
        "\n",
        "# Fetch pre-processed data for 32 keywords\n",
        "fname = fetch_file(\n",
        "    fname='kws_preprocessed_all_words_except_backward_follow_forward.pkl',\n",
        "    origin=\"https://data.brainchip.com/dataset-mirror/kws/kws_preprocessed_all_words_except_backward_follow_forward.pkl\",\n",
        "    cache_subdir='datasets/kws')\n",
        "with open(fname, 'rb') as f:\n",
        "    [_, _, x_valid, y_valid, _, _, word_to_index, _] = pickle.load(f)\n",
        "\n",
        "# Preprocessed dataset parameters\n",
        "num_classes = len(word_to_index)\n",
        "\n",
        "print(\"Wanted words and labels:\\n\", word_to_index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load a pre-trained native Keras model\n",
        "\n",
        "The model consists of:\n",
        "\n",
        "* a first convolutional layer accepting dense inputs (images),\n",
        "* several separable convolutional layers preserving spatial dimensions,\n",
        "* a global pooling reducing the spatial dimensions to a single pixel,\n",
        "* a final dense layer to classify words.\n",
        "\n",
        "All layers are followed by a batch normalization and a ReLU activation.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Retrieve the model file from the BrainChip data server\n",
        "model_file = fetch_file(fname=\"ds_cnn_kws.h5\",\n",
        "                        origin=\"https://data.brainchip.com/models/AkidaV2/ds_cnn/ds_cnn_kws.h5\",\n",
        "                        cache_subdir='models')\n",
        "\n",
        "# Load the native Keras pre-trained model\n",
        "model_keras = load_model(model_file)\n",
        "model_keras.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Check Keras Model performance\n",
        "potentials_keras = model_keras.predict(x_valid)\n",
        "preds_keras = np.squeeze(np.argmax(potentials_keras, 1))\n",
        "\n",
        "accuracy = accuracy_score(y_valid, preds_keras)\n",
        "print(\"Accuracy: \" + \"{0:.2f}\".format(100 * accuracy) + \"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Load a pre-trained quantized Keras model\n",
        "\n",
        "The above native Keras model has been quantized to 8-bit. Note that\n",
        "a 4-bit version is also available from the [model zoo](../../model_zoo_performance.html#id10).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from quantizeml.models import load_model\n",
        "\n",
        "# Load the pre-trained quantized model\n",
        "model_file = fetch_file(\n",
        "    fname=\"ds_cnn_kws_i8_w8_a8.h5\",\n",
        "    origin=\"https://data.brainchip.com/models/AkidaV2/ds_cnn/ds_cnn_kws_i8_w8_a8.h5\",\n",
        "    cache_subdir='models')\n",
        "model_keras_quantized = load_model(model_file)\n",
        "model_keras_quantized.summary()\n",
        "\n",
        "# Check Model performance\n",
        "potentials_keras_q = model_keras_quantized.predict(x_valid)\n",
        "preds_keras_q = np.squeeze(np.argmax(potentials_keras_q, 1))\n",
        "\n",
        "accuracy_q = accuracy_score(y_valid, preds_keras_q)\n",
        "print(\"Accuracy: \" + \"{0:.2f}\".format(100 * accuracy_q) + \"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Conversion to Akida\n",
        "\n",
        "The converted model is Akida 2.0 compatible and its performance\n",
        "evaluation is done using the Akida simulator.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from cnn2snn import convert\n",
        "\n",
        "# Convert the model\n",
        "model_akida = convert(model_keras_quantized)\n",
        "model_akida.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Check Akida model performance\n",
        "preds_akida = model_akida.predict_classes(x_valid, num_classes=num_classes)\n",
        "\n",
        "accuracy = accuracy_score(y_valid, preds_akida)\n",
        "print(\"Accuracy: \" + \"{0:.2f}\".format(100 * accuracy) + \"%\")\n",
        "\n",
        "# For non-regression purposes\n",
        "assert accuracy > 0.9"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Confusion matrix\n",
        "\n",
        "The confusion matrix provides a good summary of what mistakes the\n",
        "network is making.\n",
        "\n",
        "Per scikit-learn convention it displays the true class in each row (ie\n",
        "on each row you can see what the network predicted for the corresponding\n",
        "word).\n",
        "\n",
        "Please refer to the Tensorflow [audio\n",
        "recognition](https://github.com/tensorflow/docs/blob/master/site/en/r1/tutorials/sequences/audio_recognition.md#confusion-matrix)_\n",
        "example for a detailed explanation of the confusion matrix.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Create confusion matrix\n",
        "cm = confusion_matrix(y_valid, preds_akida,\n",
        "                      labels=list(word_to_index.values()))\n",
        "\n",
        "# Normalize\n",
        "cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "# Display confusion matrix\n",
        "plt.rcParams[\"figure.figsize\"] = (16, 16)\n",
        "plt.figure()\n",
        "\n",
        "title = 'Confusion matrix'\n",
        "cmap = plt.cm.Blues\n",
        "\n",
        "plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "plt.title(title)\n",
        "plt.colorbar()\n",
        "tick_marks = np.arange(len(word_to_index))\n",
        "plt.xticks(tick_marks, word_to_index, rotation=45)\n",
        "plt.yticks(tick_marks, word_to_index)\n",
        "\n",
        "thresh = cm.max() / 2.\n",
        "for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "    plt.text(j,\n",
        "             i,\n",
        "             format(cm[i, j], '.2f'),\n",
        "             horizontalalignment=\"center\",\n",
        "             color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "plt.ylabel('True label')\n",
        "plt.xlabel('Predicted label')\n",
        "plt.autoscale()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!wget https://doc.brainchipinc.com/_downloads/0792bc3dc7b01941f86b4f993c20ab5f/requirements.txt\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# AkidaNet/ImageNet inference\n",
        "\n",
        "This tutorial presents how to convert, map, and capture performance from AKD1000 Hardware using an\n",
        "AkidaNet model.\n",
        "\n",
        "AkidaNet architecture is a [MobileNet v1-inspired](https://arxiv.org/abs/1704.04861)_ architecture\n",
        "optimized for implementation on Akida 1.0: it exploits the richer expressive power of standard\n",
        "convolutions in early layers, but uses separable convolutions in later layers where filter memory is\n",
        "limiting.\n",
        "\n",
        "As [ImageNet](https://www.image-net.org/)_ images are not publicly available, performance is\n",
        "assessed using a set of 10 copyright free images that were found on Google using ImageNet class\n",
        "names.\n",
        "\n",
        ".. Note::\n",
        "    This tutorial uses an Akida 1.0 architecture to show AKD1000 mapping and performance. See the\n",
        "    [dedicated tutorial](../quantization/plot_1_upgrading_to_2.0.html)_ for 1.0 and 2.0\n",
        "    differences.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Dataset preparation\n",
        "\n",
        "Test images all have at least 256 pixels in the smallest dimension. They must\n",
        "be preprocessed to fit in the model. The ``imagenet.preprocessing.preprocess_image``\n",
        "function decodes, crops and extracts a square 224x224x3 patch from an input image.\n",
        "\n",
        ".. Note:: Input size is here set to 224x224x3 as this is what is used by the\n",
        "          model presented in the next section.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import akida\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow.io import read_file\n",
        "from tensorflow.image import decode_jpeg\n",
        "\n",
        "from akida_models import fetch_file\n",
        "from akida_models.imagenet import preprocessing\n",
        "\n",
        "# Model specification and hyperparameters\n",
        "NUM_CHANNELS = 3\n",
        "IMAGE_SIZE = 224\n",
        "\n",
        "num_images = 10\n",
        "\n",
        "# Retrieve dataset file from Brainchip data server\n",
        "file_path = fetch_file(\n",
        "    fname=\"imagenet_like.zip\",\n",
        "    origin=\"https://data.brainchip.com/dataset-mirror/imagenet_like/imagenet_like.zip\",\n",
        "    cache_subdir='datasets/imagenet_like',\n",
        "    extract=True)\n",
        "data_folder = os.path.dirname(file_path)\n",
        "\n",
        "# Load images for test set\n",
        "x_test_files = []\n",
        "x_test = np.zeros((num_images, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS)).astype('uint8')\n",
        "for id in range(num_images):\n",
        "    test_file = 'image_' + str(id + 1).zfill(2) + '.jpg'\n",
        "    x_test_files.append(test_file)\n",
        "    img_path = os.path.join(data_folder, test_file)\n",
        "    base_image = read_file(img_path)\n",
        "    image = decode_jpeg(base_image, channels=NUM_CHANNELS)\n",
        "    image = preprocessing.preprocess_image(image, (IMAGE_SIZE, IMAGE_SIZE))\n",
        "    x_test[id, :, :, :] = np.expand_dims(image, axis=0)\n",
        "\n",
        "print(f'{num_images} images loaded and preprocessed.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Labels for test images are stored in the akida_models package. The matching\n",
        "between names (*string*) and labels (*integer*) is given through the\n",
        "``imagenet.preprocessing.index_to_label`` method.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "# Parse labels file\n",
        "fname = os.path.join(data_folder, 'labels_validation.txt')\n",
        "validation_labels = dict()\n",
        "with open(fname, newline='') as csvfile:\n",
        "    reader = csv.reader(csvfile, delimiter=' ')\n",
        "    for row in reader:\n",
        "        validation_labels[row[0]] = row[1]\n",
        "\n",
        "# Get labels for the test set by index\n",
        "labels_test = np.zeros(num_images)\n",
        "for i in range(num_images):\n",
        "    labels_test[i] = int(validation_labels[x_test_files[i]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Pretrained quantized model\n",
        "\n",
        "The Akida model zoo contains a [pretrained quantized helper](../../api_reference/akida_models_apis.html#akida_models.akidanet_imagenet_pretrained).\n",
        "\n",
        "The quantization scheme for this model is the following:\n",
        "\n",
        " * the first layer has 8-bit weights,\n",
        " * all other layers have 4-bit weights,\n",
        " * all activations are 4-bit.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from cnn2snn import set_akida_version, AkidaVersion\n",
        "from akida_models import akidanet_imagenet_pretrained\n",
        "\n",
        "# Use a quantized model with pretrained quantized weights\n",
        "with set_akida_version(AkidaVersion.v1):\n",
        "    model_keras_quantized_pretrained = akidanet_imagenet_pretrained(0.5)\n",
        "model_keras_quantized_pretrained.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Check model performance on the 10 images set.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from timeit import default_timer as timer\n",
        "\n",
        "num_images = len(x_test)\n",
        "\n",
        "start = timer()\n",
        "potentials_keras = model_keras_quantized_pretrained.predict(x_test, batch_size=100)\n",
        "end = timer()\n",
        "print(f'Keras inference on {num_images} images took {end-start:.2f} s.\\n')\n",
        "\n",
        "preds_keras = np.squeeze(np.argmax(potentials_keras, 1))\n",
        "accuracy_keras = np.sum(np.equal(preds_keras, labels_test)) / num_images\n",
        "\n",
        "print(f\"Keras accuracy: {accuracy_keras*100:.2f} %\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Conversion to Akida\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1 Convert to Akida model\n",
        "\n",
        "Here, the Keras quantized model is converted into a suitable version for\n",
        "the Akida accelerator. The\n",
        "[cnn2snn.convert](../../api_reference/cnn2snn_apis.html#cnn2snn.convert)_ function only needs\n",
        "the Keras model as argument.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from cnn2snn import convert\n",
        "\n",
        "model_akida = convert(model_keras_quantized_pretrained)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The [Model.summary](../../api_reference/akida_apis.html#akida.Model.summary)_\n",
        "method provides a detailed description of the Model layers.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model_akida.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 Check performance\n",
        "\n",
        "The following will only compute accuracy for the 10 images set.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Check Model performance\n",
        "start = timer()\n",
        "accuracy_akida = model_akida.evaluate(x_test, labels_test)\n",
        "end = timer()\n",
        "print(f'Inference on {num_images} images took {end-start:.2f} s.\\n')\n",
        "print(f\"Accuracy: {accuracy_akida*100:.2f} %\")\n",
        "\n",
        "# For non-regression purposes\n",
        "assert accuracy_akida >= 0.8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.3 Show predictions for a random image\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.lines as lines\n",
        "\n",
        "\n",
        "# Functions used to display the top5 results\n",
        "def get_top5(potentials, true_label):\n",
        "    \"\"\"\n",
        "    Returns the top 5 classes from the output potentials\n",
        "    \"\"\"\n",
        "    tmp_pots = potentials.copy()\n",
        "    top5 = []\n",
        "    min_val = np.min(tmp_pots)\n",
        "    for ii in range(5):\n",
        "        best = np.argmax(tmp_pots)\n",
        "        top5.append(best)\n",
        "        tmp_pots[best] = min_val\n",
        "\n",
        "    vals = np.zeros((6,))\n",
        "    vals[:5] = potentials[top5]\n",
        "    if true_label not in top5:\n",
        "        vals[5] = potentials[true_label]\n",
        "    else:\n",
        "        vals[5] = 0\n",
        "    vals /= np.max(vals)\n",
        "\n",
        "    class_name = []\n",
        "    for ii in range(5):\n",
        "        class_name.append(preprocessing.index_to_label(top5[ii]).split(',')[0])\n",
        "    if true_label in top5:\n",
        "        class_name.append('')\n",
        "    else:\n",
        "        class_name.append(\n",
        "            preprocessing.index_to_label(true_label).split(',')[0])\n",
        "\n",
        "    return top5, vals, class_name\n",
        "\n",
        "\n",
        "def adjust_spines(ax, spines):\n",
        "    for loc, spine in ax.spines.items():\n",
        "        if loc in spines:\n",
        "            spine.set_position(('outward', 10))  # outward by 10 points\n",
        "        else:\n",
        "            spine.set_color('none')  # don't draw spine\n",
        "    # turn off ticks where there is no spine\n",
        "    if 'left' in spines:\n",
        "        ax.yaxis.set_ticks_position('left')\n",
        "    else:\n",
        "        # no yaxis ticks\n",
        "        ax.yaxis.set_ticks([])\n",
        "    if 'bottom' in spines:\n",
        "        ax.xaxis.set_ticks_position('bottom')\n",
        "    else:\n",
        "        # no xaxis ticks\n",
        "        ax.xaxis.set_ticks([])\n",
        "\n",
        "\n",
        "def prepare_plots():\n",
        "    fig = plt.figure(figsize=(8, 4))\n",
        "    # Image subplot\n",
        "    ax0 = plt.subplot(1, 3, 1)\n",
        "    imgobj = ax0.imshow(np.zeros((IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS), dtype=np.uint8))\n",
        "    ax0.set_axis_off()\n",
        "    # Top 5 results subplot\n",
        "    ax1 = plt.subplot(1, 2, 2)\n",
        "    bar_positions = (0, 1, 2, 3, 4, 6)\n",
        "    rects = ax1.barh(bar_positions, np.zeros((6,)), align='center', height=0.5)\n",
        "    plt.xlim(-0.2, 1.01)\n",
        "    ax1.set(xlim=(-0.2, 1.15), ylim=(-1.5, 12))\n",
        "    ax1.set_yticks(bar_positions)\n",
        "    ax1.invert_yaxis()\n",
        "    ax1.yaxis.set_ticks_position('left')\n",
        "    ax1.xaxis.set_ticks([])\n",
        "    adjust_spines(ax1, 'left')\n",
        "    ax1.add_line(lines.Line2D((0, 0), (-0.5, 6.5), color=(0.0, 0.0, 0.0)))\n",
        "    # Adjust Plot Positions\n",
        "    ax0.set_position([0.05, 0.055, 0.3, 0.9])\n",
        "    l1, b1, w1, h1 = ax1.get_position().bounds\n",
        "    ax1.set_position([l1 * 1.05, b1 + 0.09 * h1, w1, 0.8 * h1])\n",
        "    # Add title box\n",
        "    plt.figtext(0.5,\n",
        "                0.9,\n",
        "                \"Imagenet Classification by Akida\",\n",
        "                size=20,\n",
        "                ha=\"center\",\n",
        "                va=\"center\",\n",
        "                bbox=dict(boxstyle=\"round\",\n",
        "                          ec=(0.5, 0.5, 0.5),\n",
        "                          fc=(0.9, 0.9, 1.0)))\n",
        "\n",
        "    return fig, imgobj, ax1, rects\n",
        "\n",
        "\n",
        "def update_bars_chart(rects, vals, true_label):\n",
        "    counter = 0\n",
        "    for rect, h in zip(rects, yvals):\n",
        "        rect.set_width(h)\n",
        "        if counter < 5:\n",
        "            if top5[counter] == true_label:\n",
        "                if counter == 0:\n",
        "                    rect.set_facecolor((0.0, 1.0, 0.0))\n",
        "                else:\n",
        "                    rect.set_facecolor((0.0, 0.5, 0.0))\n",
        "            else:\n",
        "                rect.set_facecolor('gray')\n",
        "        elif counter == 5:\n",
        "            rect.set_facecolor('red')\n",
        "        counter += 1\n",
        "\n",
        "\n",
        "# Prepare plots\n",
        "fig, imgobj, ax1, rects = prepare_plots()\n",
        "\n",
        "# Get a random image\n",
        "img = np.random.randint(num_images)\n",
        "\n",
        "# Predict image class\n",
        "outputs_akida = model_akida.predict(np.expand_dims(x_test[img], axis=0)).squeeze()\n",
        "\n",
        "# Get top 5 prediction labels and associated names\n",
        "true_label = int(validation_labels[x_test_files[img]])\n",
        "top5, yvals, class_name = get_top5(outputs_akida, true_label)\n",
        "\n",
        "# Draw Plots\n",
        "imgobj.set_data(x_test[img])\n",
        "ax1.set_yticklabels(class_name, rotation='horizontal', size=9)\n",
        "update_bars_chart(rects, yvals, true_label)\n",
        "fig.canvas.draw()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Hardware mapping and performance\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.1. Map on hardware\n",
        "\n",
        "List available Akida devices and check that an NSoC V2, Akida 1.0 production chip is available.\n",
        "\n",
        "If a device is installed but not detected, reinstalling the driver might help, see the [driver\n",
        "setup helper](https://github.com/Brainchip-Inc/akida_dw_edma/blob/master/README.md)_.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "devices = akida.devices()\n",
        "print(f'Available devices: {[dev.desc for dev in devices]}')\n",
        "assert len(devices), \"No device found, this example needs an Akida NSoC_v2 device.\"\n",
        "device = devices[0]\n",
        "assert device.version == akida.NSoC_v2, \"Wrong device found, this example needs an Akida NSoC_v2.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Map the model on the device\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model_akida.map(device)\n",
        "\n",
        "# Check model mapping: NP allocation and binary size\n",
        "model_akida.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.2. Performance measurement\n",
        "\n",
        "Power measurement must be enabled on the device' soc (disabled by default).\n",
        "After sending data for inference, performance measurements are available in\n",
        "the [model statistics](../../api_reference/akida_apis.html#akida.Model.statistics)_.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Enable power measurement\n",
        "device.soc.power_measurement_enabled = True\n",
        "\n",
        "# Send data for inference\n",
        "_ = model_akida.forward(x_test)\n",
        "\n",
        "# Display floor current\n",
        "floor_power = device.soc.power_meter.floor\n",
        "print(f'Floor power: {floor_power:.2f} mW')\n",
        "\n",
        "# Retrieve statistics\n",
        "print(model_akida.statistics)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!wget https://doc.brainchipinc.com/_downloads/0792bc3dc7b01941f86b4f993c20ab5f/requirements.txt\n",
        "!pip install -r requirements.txt\n",
        "!pip install torch==2.0.1 torchvision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# PyTorch to Akida workflow\n",
        "\n",
        "The [Global Akida workflow](../general/plot_0_global_workflow.html)_ guide\n",
        "describes the steps to prepare a model for Akida starting from a TensorFlow/Keras model.\n",
        "Here we will instead describe a workflow to go from a model trained in PyTorch.\n",
        "\n",
        ".. Note::\n",
        "   | This example targets users who already have a PyTorch training pipeline\n",
        "     in place, and a trained model: this workflow will allow you to rapidly convert\n",
        "     your model to Akida 2.0.\n",
        "   | Note however that this pathway offers slightly less flexibility than our default,\n",
        "     TensorFlow-based pathway - specifically, fine tuning of the quantized model is\n",
        "     not possible when starting from PyTorch.\n",
        "   | In most cases, that won't matter, there should be almost no performance drop when\n",
        "     quantizing to 8-bit anyway.\n",
        "   | However, advanced users interested in further optimization of the original model\n",
        "     (going to 4-bit quantization for example) or users who don't yet have a\n",
        "     training pipeline in place may prefer the extra options afforded by our default,\n",
        "     TensorFlow-based [Global Akida workflow](../general/plot_0_global_workflow.html)_.\n",
        "\n",
        "\n",
        "QuantizeML natively allows the quantization and fine-tuning of TensorFlow models. While\n",
        "it does not support PyTorch quantization natively, it allows to quantize float models\n",
        "stored in the [Open Neural Network eXchange (ONNX)](https://onnx.ai)_ format. Export\n",
        "from PyTorch to ONNX is well supported, and so this provides a straightforward pathway to\n",
        "prepare your PyTorch model for Akida.\n",
        "\n",
        "As a concrete example, we will prepare a PyTorch model on a simple classification task\n",
        "(MNIST). This model will then be exported to ONNX and quantized to 8-bit using QuantizeML.\n",
        "The quantized model is then converted to Akida, and performance evaluated to show that\n",
        "there has been no loss in accuracy.\n",
        "\n",
        "Please refer to the [Akida user guide](../../user_guide/akida.html)_ for further information.\n",
        "\n",
        ".. Note::\n",
        "   | This example is loosely based on the PyTorch [Training a Classifier](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html)_ tutorial and\n",
        "     does not aim to describe PyTorch training in detail. We assume that if you are following\n",
        "     this example, it's because you already have a trained PyTorch model.\n",
        "   | [PyTorch 2.0.1](https://github.com/pytorch/pytorch/releases/tag/v2.0.1)_ is used\n",
        "     for this example.\n",
        "\n",
        "```\n",
        "pip install torch==2.0.1 torchvision\n",
        "```\n",
        ".. Warning::\n",
        "   | The MNIST example below is light enough to train on the CPU only.\n",
        "   | However, where GPU acceleration is desirable for the PyTorch training step, you may find\n",
        "     it simpler to use separate virtual environments for the PyTorch-dependent sections\n",
        "     (`1. Create and train`_ and `2. Export`_) vs the TensorFlow-dependent sections\n",
        "     (`3. Quantize`_ and `4. Convert`_).\n",
        "\n",
        "\n",
        ".. figure:: ../../img/overall_onnx_flow.png\n",
        "   :target: ../../_images/overall_onnx_flow.png\n",
        "   :alt: Overall pytorch flow\n",
        "   :scale: 60 %\n",
        "   :align: center\n",
        "\n",
        "   PyTorch Akida workflow\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Create and train\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.1. Load and normalize MNIST dataset\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "batch_size = 128\n",
        "\n",
        "\n",
        "def get_dataloader(train, batch_size, num_workers=2):\n",
        "    transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                    transforms.Normalize(0.5, 0.5)])\n",
        "    dataset = torchvision.datasets.MNIST(root='datasets/mnist',\n",
        "                                         train=train,\n",
        "                                         download=True,\n",
        "                                         transform=transform)\n",
        "    return torch.utils.data.DataLoader(dataset,\n",
        "                                       batch_size=batch_size,\n",
        "                                       shuffle=train,\n",
        "                                       num_workers=num_workers)\n",
        "\n",
        "\n",
        "# Load MNIST dataset and normalize between [-1, 1]\n",
        "trainloader = get_dataloader(train=True, batch_size=batch_size)\n",
        "testloader = get_dataloader(train=False, batch_size=batch_size)\n",
        "\n",
        "\n",
        "def imshow(img):\n",
        "    # Unnormalize\n",
        "    img = img / 2 + 0.5\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(npimg.transpose((1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Get some random training images\n",
        "images, labels = next(iter(trainloader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Show images and labels\n",
        "imshow(torchvision.utils.make_grid(images, nrow=8))\n",
        "print(\"Labels:\\n\", labels.reshape((-1, 8)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.2. Model definition\n",
        "\n",
        "Note that at this stage, there is nothing specific to the Akida IP.\n",
        "The model constructed below uses the [torch.nn.Sequential](https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html#nn-sequential)_\n",
        "module to define a standard CNN.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model_torch = torch.nn.Sequential(torch.nn.Conv2d(1, 32, 5, padding=(2, 2)),\n",
        "                                  torch.nn.ReLU6(),\n",
        "                                  torch.nn.MaxPool2d(kernel_size=2),\n",
        "                                  torch.nn.Conv2d(32, 64, 3, stride=2),\n",
        "                                  torch.nn.ReLU(),\n",
        "                                  torch.nn.Dropout(0.25),\n",
        "                                  torch.nn.Flatten(),\n",
        "                                  torch.nn.Linear(2304, 512),\n",
        "                                  torch.nn.ReLU(),\n",
        "                                  torch.nn.Dropout(0.5),\n",
        "                                  torch.nn.Linear(512, 10))\n",
        "print(model_torch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.3. Model training\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Define training rules\n",
        "optimizer = torch.optim.Adam(model_torch.parameters(), lr=1e-4)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "epochs = 10\n",
        "\n",
        "# Loop over the dataset multiple times\n",
        "for epoch in range(epochs):\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        # Get the inputs and labels\n",
        "        inputs, labels = data\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward + Backward + Optimize\n",
        "        outputs = model_torch(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Print statistics\n",
        "        running_loss += loss.detach().item()\n",
        "        if (i + 1) % 100 == 0:\n",
        "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
        "            running_loss = 0.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.4. Model testing\n",
        "\n",
        "Evaluate the model performance on the test set. It should achieve an accuracy over 98%.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        inputs, labels = data\n",
        "        # Calculate outputs by running images through the network\n",
        "        outputs = model_torch(inputs)\n",
        "        # The class with the highest score is the prediction\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "assert correct / total >= 0.98\n",
        "print(f'Test accuracy: {100 * correct // total} %')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Export\n",
        "\n",
        "PyTorch models are not directly compatible with the [QuantizeML quantization\n",
        "tool](../../api_reference/quantizeml_apis.html)_, it is therefore necessary\n",
        "to use an intermediate format. Like many other machine learning frameworks,\n",
        "PyTorch has tools to export modules in the [ONNX](https://onnx.ai)_ format.\n",
        "\n",
        "Therefore, the model is exported by the following code:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sample, _ = next(iter(trainloader))\n",
        "torch.onnx.export(model_torch,\n",
        "                  sample,\n",
        "                  f=\"mnist_cnn.onnx\",\n",
        "                  input_names=[\"inputs\"],\n",
        "                  output_names=[\"outputs\"],\n",
        "                  dynamic_axes={'inputs': {0: 'batch_size'}, 'outputs': {0: 'batch_size'}})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ".. Note::\n",
        " Find more information about how to export PyTorch models in ONNX at\n",
        " [](https://pytorch.org/docs/stable/onnx.html).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Quantize\n",
        "\n",
        "An Akida accelerator processes integer activations and weights. Therefore, the floating\n",
        "point model must be quantized in preparation to run on an Akida accelerator.\n",
        "\n",
        "The [QuantizeML quantize()](../../api_reference/quantizeml_apis.html#quantizeml.models.quantize)_\n",
        "function recognizes [ModelProto](https://onnx.ai/onnx/api/classes.html#modelproto)_ objects\n",
        "and can quantize them for Akida. The result is another ``ModelProto``, compatible with the\n",
        "[CNN2SNN Toolkit](../../user_guide/cnn2snn.html)_.\n",
        "\n",
        ".. Warning::\n",
        " ONNX and PyTorch offer their own quantization methods. You should not use those when preparing\n",
        " your model for Akida. Only the [QuantizeML quantize()](../../api_reference/quantizeml_apis.html#quantizeml.models.quantize)_ function\n",
        " can be used to generate a quantized model ready for conversion to Akida.\n",
        "\n",
        ".. Note::\n",
        " For this simple model, using random samples for calibration is sufficient, as\n",
        " shown in the following steps.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import onnx\n",
        "from quantizeml.models import quantize\n",
        "\n",
        "# Read the exported ONNX model\n",
        "model_onnx = onnx.load_model(\"mnist_cnn.onnx\")\n",
        "\n",
        "# Quantize\n",
        "model_quantized = quantize(model_onnx, num_samples=128)\n",
        "print(onnx.helper.printable_graph(model_quantized.graph))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Convert\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.1 Convert to Akida model\n",
        "\n",
        "The quantized model can now be converted to the native Akida format.\n",
        "The [convert()](../../api_reference/cnn2snn_apis.html#cnn2snn.convert)_\n",
        "function returns a model in Akida format ready for inference.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from cnn2snn import convert\n",
        "\n",
        "model_akida = convert(model_quantized)\n",
        "model_akida.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.2. Check performance\n",
        "\n",
        "Native PyTorch data must be presented in a different format to perform\n",
        "the evaluation in Akida models. Specifically:\n",
        "\n",
        "1. images must be numpy-raw, with an 8-bit unsigned integer data type and\n",
        "2. the channel dimension must be in the last dimension.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Read raw data and convert it into numpy\n",
        "x_test = testloader.dataset.data.numpy()\n",
        "y_test = testloader.dataset.targets.numpy()\n",
        "\n",
        "# Add a channel dimension to the image sets as Akida expects 4-D inputs corresponding to\n",
        "# (num_samples, width, height, channels). Note: MNIST is a grayscale dataset and is unusual\n",
        "# in this respect - most image data already includes a channel dimension, and this step will\n",
        "# not be necessary.\n",
        "x_test = x_test[..., None]\n",
        "y_test = y_test[..., None]\n",
        "\n",
        "accuracy = model_akida.evaluate(x_test, y_test)\n",
        "print('Test accuracy after conversion:', accuracy)\n",
        "\n",
        "# For non-regression purposes\n",
        "assert accuracy > 0.96"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.3 Show predictions for a single image\n",
        "\n",
        "Display one of the test images, such as the first image in the aforementioned\n",
        "dataset, to visualize the output of the model.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Test a single example\n",
        "sample_image = 0\n",
        "image = x_test[sample_image]\n",
        "outputs = model_akida.predict(image.reshape(1, 28, 28, 1))\n",
        "\n",
        "plt.imshow(x_test[sample_image].reshape((28, 28)), cmap=\"Greys\")\n",
        "print('Input Label:', y_test[sample_image].item())\n",
        "print('Prediction Label:', outputs.squeeze().argmax())"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

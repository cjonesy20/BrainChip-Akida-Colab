{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cjonesy20/BrainChip-Akida-Colab/blob/main/plot_1_akidanet_imagenet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://doc.brainchipinc.com/_downloads/0792bc3dc7b01941f86b4f993c20ab5f/requirements.txt\n",
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jo4aabDVxA4-",
        "outputId": "73f106b0-e3ad-4a4e-fe77-d3b12a10e350"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-10-09 18:47:39--  https://doc.brainchipinc.com/_downloads/0792bc3dc7b01941f86b4f993c20ab5f/requirements.txt\n",
            "Resolving doc.brainchipinc.com (doc.brainchipinc.com)... 185.199.108.153, 185.199.109.153, 185.199.110.153, ...\n",
            "Connecting to doc.brainchipinc.com (doc.brainchipinc.com)|185.199.108.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 146 [text/plain]\n",
            "Saving to: ‘requirements.txt’\n",
            "\n",
            "\rrequirements.txt      0%[                    ]       0  --.-KB/s               \rrequirements.txt    100%[===================>]     146  --.-KB/s    in 0s      \n",
            "\n",
            "2023-10-09 18:47:39 (16.0 MB/s) - ‘requirements.txt’ saved [146/146]\n",
            "\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (3.7.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (1.2.2)\n",
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (4.9.3)\n",
            "Collecting akida~=2.4.0 (from -r requirements.txt (line 7))\n",
            "  Downloading akida-2.4.0-cp310-cp310-manylinux_2_28_x86_64.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cnn2snn~=2.4.0 (from -r requirements.txt (line 8))\n",
            "  Downloading cnn2snn-2.4.0.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.2/67.2 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting akida_models~=1.2.0 (from -r requirements.txt (line 9))\n",
            "  Downloading akida_models-1.2.0.tar.gz (124 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.6/124.6 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 2)) (1.1.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 2)) (0.12.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 2)) (4.43.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 2)) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 2)) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 2)) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 2)) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 2)) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 2)) (2.8.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r requirements.txt (line 3)) (1.11.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r requirements.txt (line 3)) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r requirements.txt (line 3)) (3.2.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->-r requirements.txt (line 4)) (1.4.0)\n",
            "Requirement already satisfied: array-record in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->-r requirements.txt (line 4)) (0.4.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->-r requirements.txt (line 4)) (8.1.7)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->-r requirements.txt (line 4)) (0.1.8)\n",
            "Requirement already satisfied: etils[enp,epath,etree]>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->-r requirements.txt (line 4)) (1.5.0)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->-r requirements.txt (line 4)) (2.3)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->-r requirements.txt (line 4)) (3.20.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->-r requirements.txt (line 4)) (5.9.5)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->-r requirements.txt (line 4)) (2.31.0)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->-r requirements.txt (line 4)) (1.14.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->-r requirements.txt (line 4)) (2.3.0)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->-r requirements.txt (line 4)) (0.10.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->-r requirements.txt (line 4)) (4.66.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->-r requirements.txt (line 4)) (1.15.0)\n",
            "Collecting tensorflow<2.13.0,>=2.10.0 (from cnn2snn~=2.4.0->-r requirements.txt (line 8))\n",
            "  Downloading tensorflow-2.12.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (585.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m585.9/585.9 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting keras<2.13.0,>=2.10.0 (from cnn2snn~=2.4.0->-r requirements.txt (line 8))\n",
            "  Downloading keras-2.12.0-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m96.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting quantizeml~=0.5.0 (from cnn2snn~=2.4.0->-r requirements.txt (line 8))\n",
            "  Downloading quantizeml-0.5.3.tar.gz (70 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.7/70.7 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from akida_models~=1.2.0->-r requirements.txt (line 9)) (4.8.0.76)\n",
            "Collecting mtcnn (from akida_models~=1.2.0->-r requirements.txt (line 9))\n",
            "  Downloading mtcnn-0.1.1-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m108.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: imgaug in /usr/local/lib/python3.10/dist-packages (from akida_models~=1.2.0->-r requirements.txt (line 9)) (0.4.0)\n",
            "Collecting trimesh (from akida_models~=1.2.0->-r requirements.txt (line 9))\n",
            "  Downloading trimesh-3.23.5-py3-none-any.whl (685 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m685.4/685.4 kB\u001b[0m \u001b[31m66.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-addons>=0.18.0 (from akida_models~=1.2.0->-r requirements.txt (line 9))\n",
            "  Downloading tensorflow_addons-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (612 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m612.1/612.1 kB\u001b[0m \u001b[31m60.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets->-r requirements.txt (line 4)) (2023.6.0)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets->-r requirements.txt (line 4)) (6.1.0)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets->-r requirements.txt (line 4)) (4.5.0)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets->-r requirements.txt (line 4)) (3.17.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->-r requirements.txt (line 2)) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow-datasets->-r requirements.txt (line 4)) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow-datasets->-r requirements.txt (line 4)) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow-datasets->-r requirements.txt (line 4)) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow-datasets->-r requirements.txt (line 4)) (2023.7.22)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13.0,>=2.10.0->cnn2snn~=2.4.0->-r requirements.txt (line 8)) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13.0,>=2.10.0->cnn2snn~=2.4.0->-r requirements.txt (line 8)) (23.5.26)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13.0,>=2.10.0->cnn2snn~=2.4.0->-r requirements.txt (line 8)) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13.0,>=2.10.0->cnn2snn~=2.4.0->-r requirements.txt (line 8)) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13.0,>=2.10.0->cnn2snn~=2.4.0->-r requirements.txt (line 8)) (1.59.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13.0,>=2.10.0->cnn2snn~=2.4.0->-r requirements.txt (line 8)) (3.9.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13.0,>=2.10.0->cnn2snn~=2.4.0->-r requirements.txt (line 8)) (0.4.16)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13.0,>=2.10.0->cnn2snn~=2.4.0->-r requirements.txt (line 8)) (16.0.6)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13.0,>=2.10.0->cnn2snn~=2.4.0->-r requirements.txt (line 8)) (3.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13.0,>=2.10.0->cnn2snn~=2.4.0->-r requirements.txt (line 8)) (67.7.2)\n",
            "Collecting tensorboard<2.13,>=2.12 (from tensorflow<2.13.0,>=2.10.0->cnn2snn~=2.4.0->-r requirements.txt (line 8))\n",
            "  Downloading tensorboard-2.12.3-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m118.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-estimator<2.13,>=2.12.0 (from tensorflow<2.13.0,>=2.10.0->cnn2snn~=2.4.0->-r requirements.txt (line 8))\n",
            "  Downloading tensorflow_estimator-2.12.0-py2.py3-none-any.whl (440 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.7/440.7 kB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting wrapt (from tensorflow-datasets->-r requirements.txt (line 4))\n",
            "  Downloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13.0,>=2.10.0->cnn2snn~=2.4.0->-r requirements.txt (line 8)) (0.34.0)\n",
            "Collecting typeguard<3.0.0,>=2.7 (from tensorflow-addons>=0.18.0->akida_models~=1.2.0->-r requirements.txt (line 9))\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: scikit-image>=0.14.2 in /usr/local/lib/python3.10/dist-packages (from imgaug->akida_models~=1.2.0->-r requirements.txt (line 9)) (0.19.3)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.10/dist-packages (from imgaug->akida_models~=1.2.0->-r requirements.txt (line 9)) (2.31.5)\n",
            "Requirement already satisfied: Shapely in /usr/local/lib/python3.10/dist-packages (from imgaug->akida_models~=1.2.0->-r requirements.txt (line 9)) (2.0.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-metadata->tensorflow-datasets->-r requirements.txt (line 4)) (1.60.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<2.13.0,>=2.10.0->cnn2snn~=2.4.0->-r requirements.txt (line 8)) (0.41.2)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow<2.13.0,>=2.10.0->cnn2snn~=2.4.0->-r requirements.txt (line 8)) (0.3.1)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.14.2->imgaug->akida_models~=1.2.0->-r requirements.txt (line 9)) (3.1)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.14.2->imgaug->akida_models~=1.2.0->-r requirements.txt (line 9)) (2023.9.26)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.14.2->imgaug->akida_models~=1.2.0->-r requirements.txt (line 9)) (1.4.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13.0,>=2.10.0->cnn2snn~=2.4.0->-r requirements.txt (line 8)) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13.0,>=2.10.0->cnn2snn~=2.4.0->-r requirements.txt (line 8)) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13.0,>=2.10.0->cnn2snn~=2.4.0->-r requirements.txt (line 8)) (3.4.4)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13.0,>=2.10.0->cnn2snn~=2.4.0->-r requirements.txt (line 8)) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13.0,>=2.10.0->cnn2snn~=2.4.0->-r requirements.txt (line 8)) (3.0.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<2.13.0,>=2.10.0->cnn2snn~=2.4.0->-r requirements.txt (line 8)) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<2.13.0,>=2.10.0->cnn2snn~=2.4.0->-r requirements.txt (line 8)) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<2.13.0,>=2.10.0->cnn2snn~=2.4.0->-r requirements.txt (line 8)) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow<2.13.0,>=2.10.0->cnn2snn~=2.4.0->-r requirements.txt (line 8)) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow<2.13.0,>=2.10.0->cnn2snn~=2.4.0->-r requirements.txt (line 8)) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<2.13.0,>=2.10.0->cnn2snn~=2.4.0->-r requirements.txt (line 8)) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow<2.13.0,>=2.10.0->cnn2snn~=2.4.0->-r requirements.txt (line 8)) (3.2.2)\n",
            "Building wheels for collected packages: cnn2snn, akida_models, quantizeml\n",
            "  Building wheel for cnn2snn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for cnn2snn: filename=cnn2snn-2.4.0-py3-none-any.whl size=114399 sha256=587d62d09bffbe84b90ec63dc93051c9027d32bff898c65aa34fc59574ffd1fe\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/98/ad/e4e548669cc179725f064bea32d236178cf78a7988afd7c8d3\n",
            "  Building wheel for akida_models (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for akida_models: filename=akida_models-1.2.0-py3-none-any.whl size=172972 sha256=01e1502b99d571415fd1fa4509e9bc094823933876ae8dd983988c637ca37bc1\n",
            "  Stored in directory: /root/.cache/pip/wheels/76/06/dc/4deba7159c4fc9c0eeeede19de8190fd7a36c4a13ec8061a0d\n",
            "  Building wheel for quantizeml (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for quantizeml: filename=quantizeml-0.5.3-py3-none-any.whl size=116161 sha256=a811b6ca7047cd38ba8aafbae31adb18c4307de8b3c0673d162eae3c4cb701fb\n",
            "  Stored in directory: /root/.cache/pip/wheels/81/88/61/cddeb1338ef56af4e354e9e038b329276c45273425ab2ac51b\n",
            "Successfully built cnn2snn akida_models quantizeml\n",
            "Installing collected packages: wrapt, typeguard, trimesh, tensorflow-estimator, keras, akida, tensorflow-addons, mtcnn, tensorboard, tensorflow, quantizeml, cnn2snn, akida_models\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.15.0\n",
            "    Uninstalling wrapt-1.15.0:\n",
            "      Successfully uninstalled wrapt-1.15.0\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.13.0\n",
            "    Uninstalling tensorflow-estimator-2.13.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.13.0\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.13.1\n",
            "    Uninstalling keras-2.13.1:\n",
            "      Successfully uninstalled keras-2.13.1\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.13.0\n",
            "    Uninstalling tensorboard-2.13.0:\n",
            "      Successfully uninstalled tensorboard-2.13.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.13.0\n",
            "    Uninstalling tensorflow-2.13.0:\n",
            "      Successfully uninstalled tensorflow-2.13.0\n",
            "Successfully installed akida-2.4.0 akida_models-1.2.0 cnn2snn-2.4.0 keras-2.12.0 mtcnn-0.1.1 quantizeml-0.5.3 tensorboard-2.12.3 tensorflow-2.12.1 tensorflow-addons-0.21.0 tensorflow-estimator-2.12.0 trimesh-3.23.5 typeguard-2.13.3 wrapt-1.14.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Ma8Zr9omww9U"
      },
      "outputs": [],
      "source": [
        "%matplotlib notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZSUDBk1ww9W"
      },
      "source": [
        "\n",
        "# AkidaNet/ImageNet inference\n",
        "\n",
        "This CNN2SNN tutorial presents how to convert an AkidaNet pre-trained model into\n",
        "Akida.\n",
        "\n",
        "As ImageNet images are not publicly available, performances are assessed using a\n",
        "set of 10 copyright free images that were found on Google using ImageNet class\n",
        "names.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MW0xw2hfww9X"
      },
      "source": [
        "## 1. Dataset preparation\n",
        "\n",
        "Test images all have at least 256 pixels in the smallest dimension. They must\n",
        "be preprocessed to fit in the model. The ``imagenet.preprocessing.preprocess_image``\n",
        "function decodes, crops and extracts a square 224x224x3 patch from an input image.\n",
        "\n",
        ".. Note:: Input size is here set to 224x224x3 as this is what is used by the\n",
        "          model presented in the next section.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejzFIxMLww9Y",
        "outputId": "a8a37ecd-3859-4486-a18d-d1db385683ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from http://data.brainchip.com/dataset-mirror/imagenet_like/imagenet_like.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
            "\n",
            "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
            "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
            "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
            "\n",
            "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
            "\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20418307/20418307 [==============================] - 1s 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function preprocess_image at 0x7c6cfdd2a0e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 6 calls to <function preprocess_image at 0x7c6cfdd2a0e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10 images loaded and preprocessed.\n"
          ]
        }
      ],
      "source": [
        "import akida\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow.io import read_file\n",
        "from tensorflow.image import decode_jpeg\n",
        "from tensorflow.keras.utils import get_file\n",
        "\n",
        "from akida_models.imagenet import preprocessing\n",
        "\n",
        "# Model specification and hyperparameters\n",
        "NUM_CHANNELS = 3\n",
        "IMAGE_SIZE = 224\n",
        "NUM_CLASSES = 1000\n",
        "\n",
        "num_images = 10\n",
        "\n",
        "# Retrieve dataset file from Brainchip data server\n",
        "file_path = get_file(\n",
        "    \"imagenet_like.zip\",\n",
        "    \"http://data.brainchip.com/dataset-mirror/imagenet_like/imagenet_like.zip\",\n",
        "    cache_subdir='datasets/imagenet_like',\n",
        "    extract=True)\n",
        "data_folder = os.path.dirname(file_path)\n",
        "\n",
        "# Load images for test set\n",
        "x_test_files = []\n",
        "x_test = np.zeros((num_images, 224, 224, 3)).astype('uint8')\n",
        "for id in range(num_images):\n",
        "    test_file = 'image_' + str(id + 1).zfill(2) + '.jpg'\n",
        "    x_test_files.append(test_file)\n",
        "    img_path = os.path.join(data_folder, test_file)\n",
        "    base_image = read_file(img_path)\n",
        "    image = decode_jpeg(base_image, channels=NUM_CHANNELS)\n",
        "    image = preprocessing.preprocess_image(image, IMAGE_SIZE)\n",
        "    x_test[id, :, :, :] = np.expand_dims(image, axis=0)\n",
        "\n",
        "print(f'{num_images} images loaded and preprocessed.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVfkjuBMww9Y"
      },
      "source": [
        "Labels for test images are stored in the akida_models package. The matching\n",
        "between names (*string*) and labels (*integer*) is given through the\n",
        "``imagenet.preprocessing.index_to_label`` method.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "i05o5W44ww9Y"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "# Parse labels file\n",
        "fname = os.path.join(data_folder, 'labels_validation.txt')\n",
        "validation_labels = dict()\n",
        "with open(fname, newline='') as csvfile:\n",
        "    reader = csv.reader(csvfile, delimiter=' ')\n",
        "    for row in reader:\n",
        "        validation_labels[row[0]] = row[1]\n",
        "\n",
        "# Get labels for the test set by index\n",
        "labels_test = np.zeros(num_images)\n",
        "for i in range(num_images):\n",
        "    labels_test[i] = int(validation_labels[x_test_files[i]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_GaWt0aww9Z"
      },
      "source": [
        "## 2. Create a Keras AkidaNet model\n",
        "\n",
        "The AkidaNet architecture is available in the Akida model zoo as\n",
        "[akidanet_imagenet](../../api_reference/akida_models_apis.html#akida_models.akidanet_imagenet).\n",
        "In this tutorial, the alpha = 0.5 version of AkidaNet will be used, where\n",
        "alpha is the parameter controlling the width of the model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L9DoFwAUww9Z",
        "outputId": "c7e64043-bd0a-4df2-9136-3c7fce2acbac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from http://data.brainchip.com/models/akidanet/akidanet_imagenet_224_alpha_50.h5\n",
            "5697512/5697512 [==============================] - 1s 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"akidanet_0.50_160_1000\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input (InputLayer)          [(None, 224, 224, 3)]     0         \n",
            "                                                                 \n",
            " rescaling (Rescaling)       (None, 224, 224, 3)       0         \n",
            "                                                                 \n",
            " conv_0 (Conv2D)             (None, 112, 112, 16)      432       \n",
            "                                                                 \n",
            " conv_0/BN (BatchNormalizati  (None, 112, 112, 16)     64        \n",
            " on)                                                             \n",
            "                                                                 \n",
            " conv_0/relu (ReLU)          (None, 112, 112, 16)      0         \n",
            "                                                                 \n",
            " conv_1 (Conv2D)             (None, 112, 112, 32)      4608      \n",
            "                                                                 \n",
            " conv_1/BN (BatchNormalizati  (None, 112, 112, 32)     128       \n",
            " on)                                                             \n",
            "                                                                 \n",
            " conv_1/relu (ReLU)          (None, 112, 112, 32)      0         \n",
            "                                                                 \n",
            " conv_2 (Conv2D)             (None, 56, 56, 64)        18432     \n",
            "                                                                 \n",
            " conv_2/BN (BatchNormalizati  (None, 56, 56, 64)       256       \n",
            " on)                                                             \n",
            "                                                                 \n",
            " conv_2/relu (ReLU)          (None, 56, 56, 64)        0         \n",
            "                                                                 \n",
            " conv_3 (Conv2D)             (None, 56, 56, 64)        36864     \n",
            "                                                                 \n",
            " conv_3/BN (BatchNormalizati  (None, 56, 56, 64)       256       \n",
            " on)                                                             \n",
            "                                                                 \n",
            " conv_3/relu (ReLU)          (None, 56, 56, 64)        0         \n",
            "                                                                 \n",
            " separable_4 (SeparableConv2  (None, 28, 28, 128)      8768      \n",
            " D)                                                              \n",
            "                                                                 \n",
            " separable_4/BN (BatchNormal  (None, 28, 28, 128)      512       \n",
            " ization)                                                        \n",
            "                                                                 \n",
            " separable_4/relu (ReLU)     (None, 28, 28, 128)       0         \n",
            "                                                                 \n",
            " separable_5 (SeparableConv2  (None, 28, 28, 128)      17536     \n",
            " D)                                                              \n",
            "                                                                 \n",
            " separable_5/BN (BatchNormal  (None, 28, 28, 128)      512       \n",
            " ization)                                                        \n",
            "                                                                 \n",
            " separable_5/relu (ReLU)     (None, 28, 28, 128)       0         \n",
            "                                                                 \n",
            " separable_6 (SeparableConv2  (None, 14, 14, 256)      33920     \n",
            " D)                                                              \n",
            "                                                                 \n",
            " separable_6/BN (BatchNormal  (None, 14, 14, 256)      1024      \n",
            " ization)                                                        \n",
            "                                                                 \n",
            " separable_6/relu (ReLU)     (None, 14, 14, 256)       0         \n",
            "                                                                 \n",
            " separable_7 (SeparableConv2  (None, 14, 14, 256)      67840     \n",
            " D)                                                              \n",
            "                                                                 \n",
            " separable_7/BN (BatchNormal  (None, 14, 14, 256)      1024      \n",
            " ization)                                                        \n",
            "                                                                 \n",
            " separable_7/relu (ReLU)     (None, 14, 14, 256)       0         \n",
            "                                                                 \n",
            " separable_8 (SeparableConv2  (None, 14, 14, 256)      67840     \n",
            " D)                                                              \n",
            "                                                                 \n",
            " separable_8/BN (BatchNormal  (None, 14, 14, 256)      1024      \n",
            " ization)                                                        \n",
            "                                                                 \n",
            " separable_8/relu (ReLU)     (None, 14, 14, 256)       0         \n",
            "                                                                 \n",
            " separable_9 (SeparableConv2  (None, 14, 14, 256)      67840     \n",
            " D)                                                              \n",
            "                                                                 \n",
            " separable_9/BN (BatchNormal  (None, 14, 14, 256)      1024      \n",
            " ization)                                                        \n",
            "                                                                 \n",
            " separable_9/relu (ReLU)     (None, 14, 14, 256)       0         \n",
            "                                                                 \n",
            " separable_10 (SeparableConv  (None, 14, 14, 256)      67840     \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " separable_10/BN (BatchNorma  (None, 14, 14, 256)      1024      \n",
            " lization)                                                       \n",
            "                                                                 \n",
            " separable_10/relu (ReLU)    (None, 14, 14, 256)       0         \n",
            "                                                                 \n",
            " separable_11 (SeparableConv  (None, 14, 14, 256)      67840     \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " separable_11/BN (BatchNorma  (None, 14, 14, 256)      1024      \n",
            " lization)                                                       \n",
            "                                                                 \n",
            " separable_11/relu (ReLU)    (None, 14, 14, 256)       0         \n",
            "                                                                 \n",
            " separable_12 (SeparableConv  (None, 7, 7, 512)        133376    \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " separable_12/BN (BatchNorma  (None, 7, 7, 512)        2048      \n",
            " lization)                                                       \n",
            "                                                                 \n",
            " separable_12/relu (ReLU)    (None, 7, 7, 512)         0         \n",
            "                                                                 \n",
            " separable_13 (SeparableConv  (None, 7, 7, 512)        266752    \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " separable_13/global_avg (Gl  (None, 512)              0         \n",
            " obalAveragePooling2D)                                           \n",
            "                                                                 \n",
            " separable_13/BN (BatchNorma  (None, 512)              2048      \n",
            " lization)                                                       \n",
            "                                                                 \n",
            " separable_13/relu (ReLU)    (None, 512)               0         \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 512)               0         \n",
            "                                                                 \n",
            " classifier (Dense)          (None, 1000)              513000    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,384,856\n",
            "Trainable params: 1,378,872\n",
            "Non-trainable params: 5,984\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Retrieve the float model with pretrained weights and load it\n",
        "model_file = get_file(\n",
        "    \"akidanet_imagenet_224_alpha_50.h5\",\n",
        "    \"http://data.brainchip.com/models/akidanet/akidanet_imagenet_224_alpha_50.h5\",\n",
        "    cache_subdir='models/akidanet_imagenet')\n",
        "model_keras = load_model(model_file)\n",
        "model_keras.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmy44nnqww9Z"
      },
      "source": [
        "Top-1 accuracy on the actual ImageNet is 64.58%, the perfomance given below\n",
        "uses the 10 images subset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YPI7qqBIww9Z",
        "outputId": "041b0d29-260c-4527-f899-47d01fc677f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 372ms/step\n",
            "Keras inference on 10 images took 0.42 s.\n",
            "\n",
            "Keras accuracy: 100.00 %\n"
          ]
        }
      ],
      "source": [
        "from timeit import default_timer as timer\n",
        "\n",
        "\n",
        "# Check model performance\n",
        "def check_model_performances(model, x_test=x_test, labels_test=labels_test):\n",
        "    num_images = len(x_test)\n",
        "\n",
        "    start = timer()\n",
        "    potentials_keras = model.predict(x_test, batch_size=100)\n",
        "    end = timer()\n",
        "    print(f'Keras inference on {num_images} images took {end-start:.2f} s.\\n')\n",
        "\n",
        "    preds_keras = np.squeeze(np.argmax(potentials_keras, 1))\n",
        "    accuracy_keras = np.sum(np.equal(preds_keras, labels_test)) / num_images\n",
        "\n",
        "    print(f\"Keras accuracy: {accuracy_keras*100:.2f} %\")\n",
        "\n",
        "\n",
        "check_model_performances(model_keras)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8MtTBb08ww9a"
      },
      "source": [
        "## 3. Quantized model\n",
        "\n",
        "Quantizing a model is done using [cnn2snn.quantize](../../api_reference/cnn2snn_apis.html#quantize).\n",
        "\n",
        "The quantized model satisfies the Akida NSoC requirements:\n",
        "\n",
        " * the first layer has 8-bit weights,\n",
        " * all other convolutional layers have 4-bit weights,\n",
        " * all convolutional layers have 4-bit activations.\n",
        "\n",
        "However, this model will suffer from a drop in accuracy due to quantization\n",
        "as shown in the table below for ImageNet and in the next cell for the 10\n",
        "images set.\n",
        "\n",
        "+----------------+--------------------+\n",
        "| Float accuracy | Quantized accuracy |\n",
        "+================+====================+\n",
        "|     64.58 %    |        1.00 %      |\n",
        "+----------------+--------------------+\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e1UCYYigww9a"
      },
      "outputs": [],
      "source": [
        "from cnn2snn import quantize\n",
        "\n",
        "# Quantize the model to 4-bit weights and activations, 8-bit weights for the\n",
        "# first convolutional layer\n",
        "model_keras_quantized = quantize(model_keras, 4, 4, 8)\n",
        "\n",
        "# Check Model performance\n",
        "check_model_performances(model_keras_quantized)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKZ-t2uNww9a"
      },
      "source": [
        "## 4. Pretrained quantized model\n",
        "\n",
        "The Akida models zoo also contains a [pretrained quantized helper](../../api_reference/akida_models_apis.html#akida_models.akidanet_imagenet_pretrained)\n",
        "that was obtained after fine tuning the model for 10 epochs.\n",
        "\n",
        "Tuning the model, that is training with a lowered learning rate, allows to\n",
        "recover performances up to the initial floating point accuracy.\n",
        "\n",
        "Performances on the full ImageNet dataset are:\n",
        "\n",
        "+----------------+--------------------+--------------------+\n",
        "| Float accuracy | Quantized accuracy |     After tuning   |\n",
        "+================+====================+====================+\n",
        "|     64.58 %    |       1.00 %       |       61.30 %      |\n",
        "+----------------+--------------------+--------------------+\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3zf9nuEuww9a"
      },
      "outputs": [],
      "source": [
        "from akida_models import akidanet_imagenet_pretrained\n",
        "\n",
        "# Use a quantized model with pretrained quantized weights\n",
        "model_keras_quantized_pretrained = akidanet_imagenet_pretrained(0.5)\n",
        "model_keras_quantized_pretrained.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AWZUFKRgww9a"
      },
      "outputs": [],
      "source": [
        "# Check model performance\n",
        "check_model_performances(model_keras_quantized_pretrained)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrweFmKOww9b"
      },
      "source": [
        "## 5. Conversion to Akida\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkJcS5Cfww9b"
      },
      "source": [
        "### 5.1 Convert to Akida model\n",
        "\n",
        "Here, the Keras quantized model is converted into a suitable version for\n",
        "the Akida NSoC. The [cnn2snn.convert](../../api_reference/cnn2snn_apis.html#convert)_\n",
        "function needs as arguments the Keras model and the input scaling parameters.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BFYL_gJqww9b"
      },
      "outputs": [],
      "source": [
        "from cnn2snn import convert\n",
        "\n",
        "model_akida = convert(model_keras_quantized_pretrained)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSIy4ftyww9b"
      },
      "source": [
        "The [Model.summary](../../api_reference/akida_apis.html#akida.Model.summary)_\n",
        "method provides a detailed description of the Model layers.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IG6HUI05ww9b"
      },
      "outputs": [],
      "source": [
        "model_akida.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nfq_HcZww9b"
      },
      "source": [
        "### 5.2 Check performance\n",
        "\n",
        "While we compute accuracy for the 10 images set in the next cell, the\n",
        "following table summarizes results obtained on ImageNet.\n",
        "\n",
        "+----------------+----------------+\n",
        "| Keras accuracy | Akida accuracy |\n",
        "+================+================+\n",
        "|     61.30 %    |     61.37 %    |\n",
        "+----------------+----------------+\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y2ghy9JVww9b"
      },
      "outputs": [],
      "source": [
        "# Check Model performance\n",
        "start = timer()\n",
        "accuracy_akida = model_akida.evaluate(x_test, labels_test)\n",
        "end = timer()\n",
        "print(f'Inference on {num_images} images took {end-start:.2f} s.\\n')\n",
        "print(f\"Accuracy: {accuracy_akida*100:.2f} %\")\n",
        "\n",
        "# For non-regression purpose\n",
        "assert accuracy_akida >= 0.89"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52nLCTeGww9b"
      },
      "source": [
        "### 5.3 Show predictions for a random image\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RsNUPH-bww9c"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.lines as lines\n",
        "\n",
        "\n",
        "# Functions used to display the top5 results\n",
        "def get_top5(potentials, true_label):\n",
        "    \"\"\"\n",
        "    Returns the top 5 classes from the output potentials\n",
        "    \"\"\"\n",
        "    tmp_pots = potentials.copy()\n",
        "    top5 = []\n",
        "    min_val = np.min(tmp_pots)\n",
        "    for ii in range(5):\n",
        "        best = np.argmax(tmp_pots)\n",
        "        top5.append(best)\n",
        "        tmp_pots[best] = min_val\n",
        "\n",
        "    vals = np.zeros((6,))\n",
        "    vals[:5] = potentials[top5]\n",
        "    if true_label not in top5:\n",
        "        vals[5] = potentials[true_label]\n",
        "    else:\n",
        "        vals[5] = 0\n",
        "    vals /= np.max(vals)\n",
        "\n",
        "    class_name = []\n",
        "    for ii in range(5):\n",
        "        class_name.append(preprocessing.index_to_label(top5[ii]).split(',')[0])\n",
        "    if true_label in top5:\n",
        "        class_name.append('')\n",
        "    else:\n",
        "        class_name.append(\n",
        "            preprocessing.index_to_label(true_label).split(',')[0])\n",
        "\n",
        "    return top5, vals, class_name\n",
        "\n",
        "\n",
        "def adjust_spines(ax, spines):\n",
        "    for loc, spine in ax.spines.items():\n",
        "        if loc in spines:\n",
        "            spine.set_position(('outward', 10))  # outward by 10 points\n",
        "        else:\n",
        "            spine.set_color('none')  # don't draw spine\n",
        "    # turn off ticks where there is no spine\n",
        "    if 'left' in spines:\n",
        "        ax.yaxis.set_ticks_position('left')\n",
        "    else:\n",
        "        # no yaxis ticks\n",
        "        ax.yaxis.set_ticks([])\n",
        "    if 'bottom' in spines:\n",
        "        ax.xaxis.set_ticks_position('bottom')\n",
        "    else:\n",
        "        # no xaxis ticks\n",
        "        ax.xaxis.set_ticks([])\n",
        "\n",
        "\n",
        "def prepare_plots():\n",
        "    fig = plt.figure(figsize=(8, 4))\n",
        "    # Image subplot\n",
        "    ax0 = plt.subplot(1, 3, 1)\n",
        "    imgobj = ax0.imshow(np.zeros((224, 224, 3), dtype=np.uint8))\n",
        "    ax0.set_axis_off()\n",
        "    # Top 5 results subplot\n",
        "    ax1 = plt.subplot(1, 2, 2)\n",
        "    bar_positions = (0, 1, 2, 3, 4, 6)\n",
        "    rects = ax1.barh(bar_positions, np.zeros((6,)), align='center', height=0.5)\n",
        "    plt.xlim(-0.2, 1.01)\n",
        "    ax1.set(xlim=(-0.2, 1.15), ylim=(-1.5, 12))\n",
        "    ax1.set_yticks(bar_positions)\n",
        "    ax1.invert_yaxis()\n",
        "    ax1.yaxis.set_ticks_position('left')\n",
        "    ax1.xaxis.set_ticks([])\n",
        "    adjust_spines(ax1, 'left')\n",
        "    ax1.add_line(lines.Line2D((0, 0), (-0.5, 6.5), color=(0.0, 0.0, 0.0)))\n",
        "    # Adjust Plot Positions\n",
        "    ax0.set_position([0.05, 0.055, 0.3, 0.9])\n",
        "    l1, b1, w1, h1 = ax1.get_position().bounds\n",
        "    ax1.set_position([l1 * 1.05, b1 + 0.09 * h1, w1, 0.8 * h1])\n",
        "    # Add title box\n",
        "    plt.figtext(0.5,\n",
        "                0.9,\n",
        "                \"Imagenet Classification by Akida\",\n",
        "                size=20,\n",
        "                ha=\"center\",\n",
        "                va=\"center\",\n",
        "                bbox=dict(boxstyle=\"round\",\n",
        "                          ec=(0.5, 0.5, 0.5),\n",
        "                          fc=(0.9, 0.9, 1.0)))\n",
        "\n",
        "    return fig, imgobj, ax1, rects\n",
        "\n",
        "\n",
        "def update_bars_chart(rects, vals, true_label):\n",
        "    counter = 0\n",
        "    for rect, h in zip(rects, yvals):\n",
        "        rect.set_width(h)\n",
        "        if counter < 5:\n",
        "            if top5[counter] == true_label:\n",
        "                if counter == 0:\n",
        "                    rect.set_facecolor((0.0, 1.0, 0.0))\n",
        "                else:\n",
        "                    rect.set_facecolor((0.0, 0.5, 0.0))\n",
        "            else:\n",
        "                rect.set_facecolor('gray')\n",
        "        elif counter == 5:\n",
        "            rect.set_facecolor('red')\n",
        "        counter += 1\n",
        "\n",
        "\n",
        "# Prepare plots\n",
        "fig, imgobj, ax1, rects = prepare_plots()\n",
        "\n",
        "# Get a random image\n",
        "img = np.random.randint(num_images)\n",
        "\n",
        "# Predict image class\n",
        "potentials_akida = model_akida.predict(np.expand_dims(x_test[img],\n",
        "                                                      axis=0)).squeeze()\n",
        "\n",
        "# Get top 5 prediction labels and associated names\n",
        "true_label = int(validation_labels[x_test_files[img]])\n",
        "top5, yvals, class_name = get_top5(potentials_akida, true_label)\n",
        "\n",
        "# Draw Plots\n",
        "imgobj.set_data(x_test[img])\n",
        "ax1.set_yticklabels(class_name, rotation='horizontal', size=9)\n",
        "update_bars_chart(rects, yvals, true_label)\n",
        "fig.canvas.draw()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CerlxtaLww9c"
      },
      "source": [
        "## 6. Hardware mapping and performance\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lfBLe38ww9c"
      },
      "source": [
        "### 6.1. Map on hardware\n",
        "\n",
        "List Akida available devices and check that an NSoC V2 (production chip) is\n",
        "available\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V70HPjsFww9c"
      },
      "outputs": [],
      "source": [
        "devices = akida.devices()\n",
        "print(f'Available devices: {[dev.desc for dev in devices]}')\n",
        "device = devices[0]\n",
        "assert device.version == akida.NSoC_v2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6eZvLKPAww9c"
      },
      "source": [
        "Map the model on the device\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v9GeVoa0ww9c"
      },
      "outputs": [],
      "source": [
        "model_akida.map(device)\n",
        "\n",
        "# Check model mapping: NP allocation and binary size\n",
        "model_akida.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_G2_1VHQww9d"
      },
      "source": [
        "### 6.2. Performances measurement\n",
        "\n",
        "Power measurement must be enabled on the device' soc (disabled by default).\n",
        "After sending data for inference, performances measurements are available in\n",
        "the [model statistics](../../api_reference/akida_apis.html#akida.Model.statistics)_.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rIzT2dQSww9d"
      },
      "outputs": [],
      "source": [
        "# Enable power measurement\n",
        "device.soc.power_measurement_enabled = True\n",
        "\n",
        "# Send data for inference\n",
        "_ = model_akida.forward(x_test)\n",
        "\n",
        "# Display floor current\n",
        "floor_power = device.soc.power_meter.floor\n",
        "print(f'Floor power: {floor_power:.2f} mW')\n",
        "\n",
        "# Retrieve statistics\n",
        "print(model_akida.statistics)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
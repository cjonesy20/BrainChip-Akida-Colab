{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!wget https://doc.brainchipinc.com/_downloads/0792bc3dc7b01941f86b4f993c20ab5f/requirements.txt\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# Akida edge learning for keyword spotting\n",
        "\n",
        "This tutorial demonstrates the Akida NSoC **edge learning** capabilities using\n",
        "its built-in learning algorithm.\n",
        "\n",
        "It focuses on a keyword spotting (KWS) example, where an existing Akida network\n",
        "is re-trained to be able to classify new audio keywords.\n",
        "\n",
        "Just a few samples (few-shot learning) of the new words are sufficient to\n",
        "augment the Akida model with extra classes, while preserving high accuracy.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Edge learning process\n",
        "\n",
        "By \"edge learning\", we mean the process of network learning in an edge device.\n",
        "Aside from technical requirements imposed by the device (low power, latency,\n",
        "etc.), the task itself will often present particular challenges:\n",
        "\n",
        "1. The application cannot know which, or indeed, how many classes it will\n",
        "   be trained on ultimately, so it must be possible to **add new classes**\n",
        "   to the classifier online, i.e. requires **continual learning**.\n",
        "2. Often, there will be no large labelled dataset for new classes, which\n",
        "   must instead be learned from just a few samples, i.e. requires **few-shot\n",
        "   learning**.\n",
        "\n",
        "The Akida NSoC has a built-in learning algorithm designed for training the\n",
        "last layer of a model and well suited for edge learning.\n",
        "The specific use case in this tutorial mimics the process of a mobile\n",
        "phone user who wants to add new speech commands, i.e. new keywords, to a\n",
        "pre-trained voice recognition system with a few preset keywords.\n",
        "To achieve this using the Akida NSoC, learning occurs in 3 stages:\n",
        "\n",
        "1. The Akida model preparation: an Akida model must meet specific conditions\n",
        "   to be compatible for [Akida learning](../../user_guide/akida.html#using-akida-edge-learning)_.\n",
        "2. The \"offline\" Akida learning: the last layer of the Akida model is trained\n",
        "   from scratch with a large dataset. In this KWS case, the model is trained\n",
        "   with 32 keywords from the Google \"Speech Commands dataset\".\n",
        "3. The \"online\" (edge learning) stage: new keywords are learned with few\n",
        "   samples, adding to the pre-trained words from stage 2.\n",
        "\n",
        "\n",
        "### 1.1 Akida model preparation\n",
        "\n",
        "The Akida NSoC embeds a native learning algorithm allowing training of the\n",
        "last layer of an Akida model. The overall model can then be seen as the\n",
        "combination of two parts:\n",
        "\n",
        "- a feature extractor (or spike generator) corresponding to all but the last\n",
        "  layer of a standard (back-propagation trained) neural network. This part of\n",
        "  the model cannot be trained on the Akida NSoC, and would typically be\n",
        "  prepared in advance, e.g. using the CNN2SNN conversion tool. Also, to be\n",
        "  compatible with Akida learning, the feature extractor must return binary\n",
        "  spikes (1-bit activations).\n",
        "- the classification layer (i.e. the last layer). It must have 1-bit weights\n",
        "  and usually has several output neurons per class. This layer will be the\n",
        "  only one trained using the built-in learning algorithm.\n",
        "\n",
        "Note that, unlike a standard CNN network where each class is represented by\n",
        "a single output neuron, an Akida native training requires several neurons\n",
        "for each class. The number of neurons per class can be seen as the number\n",
        "of centroids to represent a class; there is an analogy with k-means clustering\n",
        "applied to one-class samples, k being the number of neurons. The choice of the\n",
        "number of neurons is a trade-off: too many neurons per class may be\n",
        "computationally inefficient; in contrast too few neurons per class may have\n",
        "difficulty representing within-class heterogeneity. Like k-means\n",
        "clustering, the choice of k depends on the cluster representation of the data.\n",
        "\n",
        "Like any training process, hyper-parameters must be set appropriately.\n",
        "The only mandatory parameter is the number of weights (i.e. number of\n",
        "connections for each neuron) which must be correlated to the number of spikes\n",
        "at the end of the feature extractor. Other parameters, such as\n",
        "``min_plasticity`` or ``learning_competition``, are optional and mainly used\n",
        "for model fine-tuning: one can set them to default for a first try.\n",
        "\n",
        "\n",
        "### 1.2 \"Offline\" Akida learning\n",
        "\n",
        "The model is ready for training. Remember that the feature extractor has\n",
        "already been trained in stage 1. Here, only the last Akida layer is\n",
        "trainable. Training is still \"offline\" though, corresponding to the\n",
        "preparation of the network with the \"preset\" command keywords. The last layer\n",
        "is trained from scratch: its binary weights are randomly initialized.\n",
        "\n",
        "A large dataset is passed through the Akida network and the on-chip learning\n",
        "algorithm updates the weights of the classification layer accordingly.\n",
        "In this KWS example, we take a dataset containing 32 words + a \"silence\"\n",
        "class (33 classes) for a total of about 94,000 samples.\n",
        "\n",
        "Note that the dataset on which the feature extractor was trained does not need\n",
        "to be the same as the one used for \"offline\" training of the classification\n",
        "layer. What is important is that the features extracted are as good as\n",
        "possible for the expected inputs. Since the \"edge\" classes are, by\n",
        "definition, not known in advance, in practice that typically means making\n",
        "your feature extractor as general as possible.\n",
        "\n",
        "\n",
        "### 1.3 \"Online\" edge learning\n",
        "\n",
        "\"Online\" edge learning consists in adding and learning new classes to a\n",
        "former pre-trained model. This stage is meant to be performed on a chip with\n",
        "few examples for each new class.\n",
        "\n",
        "In practice, edge learning with Akida is similar to \"offline\" learning,\n",
        "except that:\n",
        "\n",
        "- the network has already been trained on a set of classes which need to be\n",
        "  kept, and so the novel classes are in addition to those.\n",
        "- only few samples are available for training.\n",
        "\n",
        "In this KWS example, 3 new keywords are learned using 4 samples per word from\n",
        "a single user. Applying data augmentation on these samples adds variability\n",
        "to improve generalization. After edge learning, the model is able to classify\n",
        "the 3 new classes with similar accuracy to the 33 existing classes (and\n",
        "performance on the existing classes is unaffected).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Dataset preparation\n",
        "\n",
        "The data comes from the Google \"Speech Commands\" dataset containing audio\n",
        "files for 35 keywords. The number of utterances for each word varies from\n",
        "1,500 to 4,000.\n",
        "32 words are used to train the Akida model and 3 new words are added for\n",
        "edge learning.\n",
        "\n",
        "Two datasets are loaded:\n",
        "\n",
        "- The first dataset contains all samples of the 32 following keywords\n",
        "  extended with the \"silence\" samples (see the\n",
        "  [original paper](https://arxiv.org/pdf/1804.03209.pdf)_ for details on\n",
        "  the dataset). In total, 94,252 samples are used. These are split into a\n",
        "  training set (90%) and a validation set (10%), used to train the model\n",
        "  \"offline\" (stage 2).\n",
        "- The second dataset contains samples of the 3 new keywords from a single\n",
        "  speaker: 'backward', 'follow' and 'forward'. Since the aim of edge learning\n",
        "  is to train with few samples, only 4 utterances will be used for\n",
        "  training and the rest for testing (ideally, one would test with many more\n",
        "  samples, but the number of repetitions per individual speaker in the\n",
        "  database makes this impossible). Data augmentation is applied with time\n",
        "  shift and additional background noise, generating 40 training samples per\n",
        "  utterances, therefore 4 x 40 = 160 training samples per new word.\n",
        "\n",
        "The audio files are pre-processed: the mel-frequency cepstral coefficients\n",
        "(MFCC) are computed as features to represent each audio sample. The obtained\n",
        "features for one sample are stored in an array of shape (49, 10, 1). This\n",
        "array of features is chosen as input in the Akida network.\n",
        "\n",
        "For the sake of simplicity, the pre-processing step is not detailed here;\n",
        "this tutorial directly fetches the pre-processed audio data for both datasets.\n",
        "The pre-processed utility methods to generate these MFCC data are available in\n",
        "the ``akida_models`` package.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from akida import FullyConnected, evaluate_sparsity, Model\n",
        "\n",
        "import pickle\n",
        "\n",
        "from akida_models import fetch_file\n",
        "\n",
        "# Fetch pre-processed data for 32 keywords\n",
        "fname = fetch_file(\n",
        "    fname='kws_preprocessed_all_words_except_backward_follow_forward.pkl',\n",
        "    origin=\"https://data.brainchip.com/dataset-mirror/kws/kws_preprocessed_all_words_except_backward_follow_forward.pkl\",\n",
        "    cache_subdir='datasets/kws')\n",
        "with open(fname, 'rb') as f:\n",
        "    [x_train, y_train, x_val, y_val, _, _, word_to_index,\n",
        "     data_transform] = pickle.load(f)\n",
        "\n",
        "# Fetch pre-processed data for the 3 new keywords\n",
        "fname2 = fetch_file(\n",
        "    fname='kws_preprocessed_edge_backward_follow_forward.pkl',\n",
        "    origin=\"https://data.brainchip.com/dataset-mirror/kws/kws_preprocessed_edge_backward_follow_forward.pkl\",\n",
        "    cache_subdir='datasets/kws')\n",
        "with open(fname2, 'rb') as f:\n",
        "    [\n",
        "        x_train_new, y_train_new, x_val_new, y_val_new, files_train, files_val,\n",
        "        word_to_index_new, dt2\n",
        "    ] = pickle.load(f)\n",
        "\n",
        "print(\"Wanted words and labels:\\n\", word_to_index)\n",
        "print(\"New words:\\n\", word_to_index_new)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Prepare Akida model for learning\n",
        "\n",
        "As explained above, to be compatible with Akida:\n",
        "\n",
        "- the feature extractor must return **binary spikes**.\n",
        "- the classification layer must have **binary weights**.\n",
        "\n",
        "For this example, we load a pre-trained model from which we keep the feature\n",
        "extractor, returning binary spikes. This model was previously trained and\n",
        "quantized with Keras and the CNN2SNN tool. The first dataset with 33 classes\n",
        "(32 keywords + \"silence\") was used for training.\n",
        "\n",
        "However, the last layer of this pre-trained model is not compatible for Akida\n",
        "learning since it doesn't have binary weights. We then remove this last layer\n",
        "and add a new classification layer with 33 classes and\n",
        "15 neurons per class. One can try with different values of neurons per\n",
        "class, e.g. from 1 to 500 neurons per class, and see the effects on\n",
        "performance and time cost.\n",
        "\n",
        "Moreover, as for any training algorithm, the learning hyper-parameters have to\n",
        "be correctly set. For the Akida learning algorithm, one important parameter\n",
        "is the **number of weights**: because of the way the Akida learning algorithm\n",
        "works, the number of spikes at the end of the feature extractor provides a\n",
        "good starting point for this hyper-parameter. Here, we estimate this number\n",
        "of output spikes using 10% of the training set, which is enough to have a\n",
        "reasonable estimation.\n",
        "\n",
        ".. Note:: Edge learning is only supported for Akida 1.0 models for now.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from akida_models import ds_cnn_kws_pretrained\n",
        "from cnn2snn import set_akida_version, AkidaVersion\n",
        "\n",
        "# Instantiate a quantized model with pretrained quantized weights\n",
        "with set_akida_version(AkidaVersion.v1):\n",
        "    model = ds_cnn_kws_pretrained()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "from math import ceil\n",
        "\n",
        "from cnn2snn import convert\n",
        "\n",
        "#  Convert to an Akida model\n",
        "model_ak = convert(model)\n",
        "model_ak.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Measure Akida accuracy on validation set\n",
        "batch_size = 1000\n",
        "preds_ak = np.zeros(y_val.shape[0])\n",
        "num_batches_val = ceil(x_val.shape[0] / batch_size)\n",
        "for i in range(num_batches_val):\n",
        "    s = slice(i * batch_size, (i + 1) * batch_size)\n",
        "    preds_ak[s] = model_ak.predict_classes(x_val[s])\n",
        "\n",
        "acc_val_ak = np.sum(preds_ak == y_val) / y_val.shape[0]\n",
        "print(f\"Akida CNN2SNN validation set accuracy: {100 * acc_val_ak:.2f} %\")\n",
        "\n",
        "# For non-regression purposes\n",
        "assert acc_val_ak > 0.88"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Replace the last layer by a classification layer with binary weights\n",
        "# Here, we choose to set 15 neurons per class.\n",
        "num_classes = 33\n",
        "num_neurons_per_class = 15\n",
        "\n",
        "model_ak.pop_layer()\n",
        "layer_fc = FullyConnected(name='akida_edge_layer',\n",
        "                          units=num_classes * num_neurons_per_class,\n",
        "                          activation=False)\n",
        "model_ak.add(layer_fc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Compute sparsity information for the model using 10% of the training data\n",
        "# which is enough for a good estimate\n",
        "num_samples = ceil(0.1 * x_train.shape[0])\n",
        "sparsities = evaluate_sparsity(model_ak, x_train[:num_samples])\n",
        "\n",
        "# Retrieve the number of output spikes from the feature extractor output\n",
        "output_density = 1 - sparsities[model_ak.get_layer('separable_4')]\n",
        "avg_spikes = model_ak.get_layer('separable_4').output_dims[-1] * output_density\n",
        "print(f\"Average number of spikes: {avg_spikes}\")\n",
        "\n",
        "# Fix the number of weights to 1.2 times the average number of output spikes\n",
        "num_weights = int(1.2 * avg_spikes)\n",
        "print(\"The number of weights is then set to:\", num_weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Learn with Akida using the training set\n",
        "\n",
        "This stage shows how to train the Akida model using the built-in learning\n",
        "algorithm in an \"offline\" stage, i.e. training the classification layer\n",
        "from scratch using a large training set.\n",
        "The dataset containing the 33 classes (32 keywords + \"silence\") is used.\n",
        "\n",
        "Now that the Akida model is ready for training, the hyper-parameters\n",
        "must be set using the [compile](../../api_reference/akida_apis.html#akida.Model.compile)_\n",
        "method of the last layer. Compiling a layer means that this layer is\n",
        "configured for training and ready to be trained. For more information about\n",
        "the learning hyper-parameters, check the [user guide](../../user_guide/akida.html#compiling-a-layer)_.\n",
        "Note that we set the `learning_competition` to 0.1, which gives a little\n",
        "competition between neurons to prevent learning similar features.\n",
        "\n",
        "Once the last layer is compiled, the\n",
        "[fit](../../api_reference/akida_apis.html#akida.Model.fit) method is used to\n",
        "pass the dataset for training. This call is similar to the `fit` method in\n",
        "tf.keras.\n",
        "\n",
        "After training, the model is assessed on the validation set using the\n",
        "[predict](../../api_reference/akida_apis.html#akida.Model.predict) method. It\n",
        "returns the estimated labels for the validation samples.\n",
        "The model is then saved to a ``.fbz`` file.\n",
        "\n",
        "Note that in this specific case, the same dataset was used to train the\n",
        "feature extractor using the CNN2SNN tool in an early stage, and to train this\n",
        "classification layer using the native learning algorithm. However, the edge\n",
        "learning in the next stage passes completely new data in the network.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Compile Akida model with learning parameters\n",
        "from akida import AkidaUnsupervised\n",
        "model_ak.compile(optimizer=AkidaUnsupervised(num_weights=num_weights,\n",
        "                                             num_classes=num_classes,\n",
        "                                             learning_competition=0.1))\n",
        "model_ak.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from time import time\n",
        "\n",
        "# Train the last layer using Akida `fit` method\n",
        "print(f\"Akida learning with {num_classes} classes... \\\n",
        "        (this step can take a few minutes)\")\n",
        "num_batches = ceil(x_train.shape[0] / batch_size)\n",
        "start = time()\n",
        "for i in range(num_batches):\n",
        "    s = slice(i * batch_size, (i + 1) * batch_size)\n",
        "    model_ak.fit(x_train[s], y_train[s].astype(np.int32))\n",
        "end = time()\n",
        "\n",
        "print(f\"Elapsed time for Akida training: {end-start:.2f} s\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Measure Akida accuracy on validation set\n",
        "preds_val_ak = np.zeros(y_val.shape[0])\n",
        "for i in range(num_batches_val):\n",
        "    s = slice(i * batch_size, (i + 1) * batch_size)\n",
        "    preds_val_ak[s] = model_ak.predict_classes(x_val[s],\n",
        "                                               num_classes=num_classes)\n",
        "\n",
        "acc_val_ak = np.sum(preds_val_ak == y_val) / y_val.shape[0]\n",
        "print(f\"Akida validation set accuracy: {100 * acc_val_ak:.2f} %\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "from tempfile import TemporaryDirectory\n",
        "\n",
        "# Save Akida model\n",
        "temp_dir = TemporaryDirectory(prefix='edge_learning_kws')\n",
        "model_file = os.path.join(temp_dir.name, 'ds_cnn_edge_kws.fbz')\n",
        "model_ak.save(model_file)\n",
        "del model_ak"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Edge learning\n",
        "\n",
        "After the \"offline\" training stage, we emulate the use case where the\n",
        "pre-trained Akida model is loaded on an Akida chip, ready to learn new\n",
        "classes. Our previously saved Akida model has 33 output classes with learned\n",
        "weights.\n",
        "We now add 3 classes to the existing model using the\n",
        "[add_classes](../../api_reference/akida_apis.html#akida.Model.add_classes) method\n",
        "and learn the 3 new keywords without changing the already learned weights.\n",
        "\n",
        "There is no need to compile the final layer again; the new neurons were\n",
        "initialized along with the old ones, based on the learning hyper-parameters\n",
        "given in the [compile](../../api_reference/akida_apis.html#akida.Model.compile)\n",
        "call. The edge learning then uses the same scheme as for the \"offline\" Akida\n",
        "learning - only the number of samples used is much more restricted.\n",
        "\n",
        "Here, each new class is trained using 160 samples, stored in the second\n",
        "dataset: 4 utterances per word from a single speaker, augmented 40 times each.\n",
        "The validation set for new words ['backward', 'follow', 'forward'] contains\n",
        "respectively 6, 7 and 6 utterances.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(f\"Validation set of new words ({y_val_new.shape[0]} samples):\")\n",
        "for word, label in word_to_index_new.items():\n",
        "    print(f\" - {word} (label {label}): {np.sum(y_val_new == label)} samples\")\n",
        "\n",
        "# Update new labels following the numbering of the old keywords, i.e, new word\n",
        "# with label '0' becomes label '34', new word label '1' becomes '35', etc.\n",
        "y_train_new += num_classes\n",
        "y_val_new += num_classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Load the pre-trained model (no need to compile it again)\n",
        "model_edge = Model(model_file)\n",
        "model_edge.add_classes(3)\n",
        "\n",
        "# Train the Akida model with new keywords; only few samples are used.\n",
        "print(\"\\nEdge learning with 3 new classes ...\")\n",
        "start = time()\n",
        "model_edge.fit(x_train_new, y_train_new.astype(np.int32))\n",
        "end = time()\n",
        "print(f\"Elapsed time for Akida edge learning: {end-start:.2f} s\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Predict on the new validation set\n",
        "preds_ak_new = model_edge.predict_classes(x_val_new,\n",
        "                                          num_classes=num_classes + 3)\n",
        "good_preds_val_new_ak = np.sum(preds_ak_new == y_val_new)\n",
        "print(f\"Akida validation set accuracy on 3 new keywords: \\\n",
        "        {good_preds_val_new_ak}/{y_val_new.shape[0]}\")\n",
        "\n",
        "# Predict on the old validation set. Edge learning of the 3 new keywords barely\n",
        "# affects the accuracy of the old classes.\n",
        "preds_ak_old = np.zeros(y_val.shape[0])\n",
        "for i in range(num_batches_val):\n",
        "    s = slice(i * batch_size, (i + 1) * batch_size)\n",
        "    preds_ak_old[s] = model_edge.predict_classes(x_val[s],\n",
        "                                                 num_classes=num_classes + 3)\n",
        "\n",
        "acc_val_old_ak = np.sum(preds_ak_old == y_val) / y_val.shape[0]\n",
        "print(f\"Akida validation set accuracy on 33 old classes: \\\n",
        "        {100 * acc_val_old_ak:.2f} %\")\n",
        "\n",
        "# For non-regression purposes\n",
        "assert acc_val_old_ak > 0.82"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

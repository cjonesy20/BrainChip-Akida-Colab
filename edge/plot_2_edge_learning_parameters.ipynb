{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!wget https://doc.brainchipinc.com/_downloads/0792bc3dc7b01941f86b4f993c20ab5f/requirements.txt\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# Tips to set Akida edge learning parameters\n",
        "\n",
        "This tutorial gives details about the Akida learning parameters and tips to\n",
        "set their values in a first try in an edge learning application. The KWS dataset\n",
        "and the DS-CNN-edge model are used as a classification example to showcase the\n",
        "handy tips.\n",
        "\n",
        "One can consult the [KWS edge learning tutorial](plot_1_edge_learning_kws.html)\n",
        "for a first approach about Akida learning.\n",
        "\n",
        ".. Note:: The hints given in this tutorial are not a promise to get the best\n",
        "          performance. They can be seen as an initialization, before\n",
        "          fine-tuning. Besides, even if these tips provide good estimates in\n",
        "          most examples, they can't be guaranteed to work for every application.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Akida learning parameters\n",
        "\n",
        "To be ready for learning, an Akida model must be composed of:\n",
        "\n",
        "  1. a feature extractor returning binary spikes: this part is usually trained\n",
        "     using the CNN2SNN toolkit.\n",
        "  2. an Akida trainable layer added on top of the feature extractor: it must\n",
        "     have 1-bit weights and several output neurons per class.\n",
        "\n",
        "The last trainable layer must be correctly configured to get good learning\n",
        "performance. The two main parameters to set are:\n",
        "\n",
        "  - the number of weights\n",
        "  - the number of neurons per class\n",
        "\n",
        "In the next sections, details about these hyper-parameters are given with\n",
        "handy tips to give a first estimation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Create Akida model\n",
        "\n",
        "In a first stage, we will create the Akida feature extractor returning\n",
        "binary spikes. From then, we will be able to estimate the parameters for\n",
        "the trainable layer that will be added later.\n",
        "\n",
        "After loading the KWS dataset, we create the pre-trained Keras model and\n",
        "convert it to an Akida model. We then remove the last layer to get the\n",
        "feature extractor.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from akida import Model, InputData, FullyConnected\n",
        "\n",
        "import pickle\n",
        "\n",
        "from akida_models import fetch_file\n",
        "\n",
        "# Fetch pre-processed data for 32 keywords\n",
        "fname = fetch_file(\n",
        "    fname='kws_preprocessed_all_words_except_backward_follow_forward.pkl',\n",
        "    origin=\"https://data.brainchip.com/dataset-mirror/kws/kws_preprocessed_all_words_except_backward_follow_forward.pkl\",\n",
        "    cache_subdir='datasets/kws')\n",
        "with open(fname, 'rb') as f:\n",
        "    [x_train, y_train, _, _, _, _, word_to_index, _] = pickle.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ".. Note:: Edge learning is only supported for Akida 1.0 models for now.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from cnn2snn import convert, set_akida_version, AkidaVersion\n",
        "from akida_models import ds_cnn_kws_pretrained\n",
        "\n",
        "# Instantiate a quantized model with pretrained quantized weights\n",
        "with set_akida_version(AkidaVersion.v1):\n",
        "    model = ds_cnn_kws_pretrained()\n",
        "\n",
        "# Convert to an Akida model\n",
        "model_ak = convert(model)\n",
        "\n",
        "# Remove last layer\n",
        "model_ak.pop_layer()\n",
        "model_ak.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Estimate the required number of weights of the trainable layer\n",
        "\n",
        "The number of weights corresponds to the number of connections for each\n",
        "neuron. The smaller the number of weights, the less specific the neurons will\n",
        "be. Setting this parameter correctly is important.\n",
        "\n",
        "Although the last trainable layer hasn't been created yet, we can already\n",
        "estimate the number of weights. This estimation is based on the statistics of\n",
        "the output spikes from the feature extractor. Intuitively, a sample producing\n",
        "N spikes at the end of the feature extractor could be perfectly represented\n",
        "with a neuron with N weights. We then use the median of the number of output\n",
        "spikes for all samples.\n",
        "\n",
        "To reduce computing time, using only a subset of the whole dataset may be\n",
        "sufficient to get an estimation of the number of spikes. We then set the\n",
        "number of weights to a value sligthly higher than the median of the number of\n",
        "spikes: we generally choose 1.2 x median of number of spikes, which seems to\n",
        "give good results.\n",
        "\n",
        "For a deeper analysis of the output spikes from the feature extractor, one\n",
        "could look at the distribution of the number of spikes, either for all samples\n",
        "or for samples of each class separately. This analysis is not shown here.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Forward samples to get the number of output spikes\n",
        "# Here, 10% of the training set is sufficient for a good estimation\n",
        "num_samples_to_use = int(len(x_train) / 10)\n",
        "spikes = model_ak.forward(x_train[:num_samples_to_use])\n",
        "\n",
        "# Compute the median of the number of output spikes\n",
        "median_spikes = np.median(spikes.sum(axis=(1, 2, 3)))\n",
        "print(f\"Median of number of spikes: {median_spikes}\")\n",
        "\n",
        "# Set the number of weights to 1.2 x median\n",
        "num_weights = int(1.2 * median_spikes)\n",
        "print(f\"The number of weights is then set to: {num_weights}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Estimate the number of neurons per class\n",
        "\n",
        "Unlike a standard CNN network where each class is represented by a single\n",
        "output neuron, an Akida native training requires several neurons for each\n",
        "class to better represent the class variability. Choosing the right number of\n",
        "neurons per class is a trade-off between enough neurons to represent the\n",
        "classes' variabilities, but not too many neurons implying more memory and\n",
        "computing time. This is similar to clustering algorithms where the clusters\n",
        "represent the distribution of the data. Note that, like clustering algorithms,\n",
        "this analysis requires to have more samples per class than the number of\n",
        "neurons per class: only one neuron can learn per sample. Having more neurons\n",
        "than samples, the extra neurons are guaranteed to be wasted.\n",
        "\n",
        "One direct option is to train the classification layer using the whole dataset\n",
        "with different values of number of neurons per class. Looking at the\n",
        "validation accuracy, it should increase with more neurons per class, then\n",
        "reach a plateau where adding more neurons has very small effect. Choosing the\n",
        "value where the accuracy begins to flatten is a good estimation.\n",
        "\n",
        "However, this method is very time consuming since it requires multiple\n",
        "trainings using the whole dataset. Another option is to only train on a few\n",
        "number of classes. Rather than measuring accuracy, we measure the error\n",
        "between the potential of the matching neuron and the maximum theoretical\n",
        "potential. Taking a simple example:\n",
        "\n",
        "- Let's say 3 neurons per class, with 180 weights\n",
        "- A sample of a given class returns 3 potentials for the 3 neurons of its\n",
        "  class: [12, 153, 97]. The maximum potential is 153.\n",
        "- The error between the sample and the neuron is 180 - 153 = 27.\n",
        "- Compute the loss being the sum of the errors for all samples of a class.\n",
        "\n",
        "Visualizing the loss for a given class as a function of the number of neurons\n",
        "gives hints to have a first estimation of the number of neurons per class.\n",
        "Visualizing the number of neurons that have learned as a function of the\n",
        "number of neurons per class provides a similar analysis.\n",
        "\n",
        "In this tutorial, we only present this analysis for one class (word 'six').\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from akida import AkidaUnsupervised\n",
        "\n",
        "\n",
        "def compute_losses(model,\n",
        "                   samples,\n",
        "                   neurons_per_class,\n",
        "                   num_weights,\n",
        "                   learning_competition=0.1,\n",
        "                   num_repetitions=1):\n",
        "    \"\"\"Compute losses after training an Akida FullyConnected layer for samples\n",
        "    of one class.\n",
        "\n",
        "    For each value of 'neurons_per_class', a training is performed, and the loss\n",
        "    and the number of neurons that have learned are returned.\n",
        "\n",
        "    Args:\n",
        "        model: an Akida model for feature extraction\n",
        "        samples: a NumPy array of input samples of one class\n",
        "        neurons_per_class: an 1-D iterable object storing the integer values of\n",
        "            the number of neurons to test\n",
        "        num_weights: the number of non-zero weights in each neuron\n",
        "        learning_competition: the learning competition of the trainable layer\n",
        "        num_repetitions: the number of times the training must be performed.\n",
        "            The training with the minimum loss will be kept.\n",
        "\n",
        "    Returns:\n",
        "        the losses and the numbers of neurons that have learned\n",
        "\n",
        "    \"\"\"\n",
        "    spikes = model.forward(samples)\n",
        "\n",
        "    def create_one_fc_model(units):\n",
        "        model_fc = Model()\n",
        "        model_fc.add(\n",
        "            InputData(name=\"input\",\n",
        "                      input_shape=(1, 1, spikes.shape[-1]),\n",
        "                      input_bits=1))\n",
        "        layer_fc = FullyConnected(name='akida_edge_layer',\n",
        "                                  units=units,\n",
        "                                  activation=False)\n",
        "        model_fc.add(layer_fc)\n",
        "        model_fc.compile(optimizer=AkidaUnsupervised(num_weights=num_weights,\n",
        "                                                     learning_competition=learning_competition))\n",
        "        return model_fc\n",
        "\n",
        "    losses = np.zeros((len(neurons_per_class), num_repetitions))\n",
        "    num_learned_neurons = np.zeros((len(neurons_per_class), num_repetitions))\n",
        "    for idx, n in enumerate(neurons_per_class):\n",
        "        for i in range(num_repetitions):\n",
        "            model_fc = create_one_fc_model(units=n)\n",
        "\n",
        "            # Train model\n",
        "            permut_spikes = np.random.permutation(spikes)\n",
        "            model_fc.fit(permut_spikes)\n",
        "\n",
        "            # Get max potentials\n",
        "            max_potentials = model_fc.forward(permut_spikes).max(axis=-1)\n",
        "            losses[idx, i] = np.sum(num_weights - max_potentials)\n",
        "\n",
        "            # Get threshold learning\n",
        "            th_learn = model_fc.get_layer('akida_edge_layer').get_variable(\n",
        "                'threshold_learning')\n",
        "            num_learned_neurons[idx, i] = np.sum(th_learn > 0)\n",
        "\n",
        "    return losses.min(axis=1) / len(spikes), num_learned_neurons.min(axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Choose a word to analyze and the values for the number of neurons\n",
        "word = 'six'\n",
        "neurons_per_class = [\n",
        "    2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20, 25, 30, 35, 40, 45, 50, 60, 70, 80, 90,\n",
        "    100, 150, 200, 250, 300, 350, 400, 450, 500, 750, 1000\n",
        "]\n",
        "\n",
        "# Compute the losses for word 'six' and different number of neurons\n",
        "idx_samples = (y_train == word_to_index[word])\n",
        "x_train_word = x_train[idx_samples]\n",
        "losses, num_learned_neurons = compute_losses(model_ak, x_train_word,\n",
        "                                             neurons_per_class, num_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(np.array(neurons_per_class), losses)\n",
        "plt.xlabel(\"Number of neurons per class\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(f\"Losses for samples of class '{word}'\")\n",
        "plt.grid(linestyle='--')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plt.plot(np.array(neurons_per_class), num_learned_neurons)\n",
        "plt.xlabel(\"Number of neurons per class\")\n",
        "plt.ylabel(\"Nb of neurons that have learned\")\n",
        "plt.title(f\"Nb of neurons that have learned for samples of class '{word}'\")\n",
        "plt.grid(linestyle='--')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From the figures above, we can see that the point of inflection occured with\n",
        "about 300 neurons. Setting the number of neurons per class to this value is a\n",
        "good starting point: we expect a very good accuracy after training. Adding\n",
        "more neurons won't improve the acccuracy and will increase the computing time.\n",
        "\n",
        "However, one could gradually reduce the number of neurons per class to see its\n",
        "influence on the accuracy of a complete training. In the KWS edge tutorial, we\n",
        "finally set this value to 50 because it is a good trade-off between computing\n",
        "time and our target accuracy. The table below presents the validation accuracy\n",
        "after training for different numbers of neurons. We can see that there is no\n",
        "increase for a number of neurons per class higher than 300. Note that in this\n",
        "use case, the validation accuracy remains very high even for a small number of\n",
        "neurons per class: one should be aware that this small decrease in accuracy\n",
        "cannot be generalized for all use cases.\n",
        "\n",
        "+-------------+----------+-------------+\n",
        "| Nb. neurons | Accuracy | Time ratio  |\n",
        "+=============+==========+=============+\n",
        "| 10          | 91.6 %   | 0.83        |\n",
        "+-------------+----------+-------------+\n",
        "| 20          | 91.8 %   | 0.84        |\n",
        "+-------------+----------+-------------+\n",
        "| 50          | 92.1 %   | 0.86        |\n",
        "+-------------+----------+-------------+\n",
        "| 100         | 92.3 %   | 0.89        |\n",
        "+-------------+----------+-------------+\n",
        "| 200         | 92.5 %   | 0.94        |\n",
        "+-------------+----------+-------------+\n",
        "| 300         | 92.6 %   | 1           |\n",
        "+-------------+----------+-------------+\n",
        "| 400         | 92.5 %   | 1.05        |\n",
        "+-------------+----------+-------------+\n",
        "| 500         | 92.5 %   | 1.10        |\n",
        "+-------------+----------+-------------+\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
